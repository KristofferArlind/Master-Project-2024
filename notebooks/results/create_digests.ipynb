{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import sys\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df76894",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = \"trr_5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b37aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_market_caps(results_df, min_market_cap_percentile = 0.6, max_market_cap_percentile = None):\n",
    "    current_results = results_df.copy()\n",
    "    \n",
    "    min_market_caps = current_results.groupby(\"date\")[\"market_cap_usd\"].quantile(min_market_cap_percentile)\n",
    "    \n",
    "    if max_market_cap_percentile != None:\n",
    "        max_market_caps = current_results.groupby(\"date\")[\"market_cap_usd\"].quantile(max_market_cap_percentile)\n",
    "           \n",
    "    current_results = current_results.groupby(\"date\").apply(lambda x: x[x[\"market_cap_usd\"] >= min_market_caps.loc[x.name]]).reset_index(drop=True)\n",
    "\n",
    "    if max_market_cap_percentile != None:\n",
    "        current_results = current_results.groupby(\"date\").apply(lambda x: x[x[\"market_cap_usd\"] <= max_market_caps.loc[x.name]]).reset_index(drop=True)\n",
    "    current_results.sort_values([\"date\", \"gvkey\"], inplace=True)\n",
    "\n",
    "    return current_results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3be3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_quantiles(results_df, quantiles=10):\n",
    "    results_df = results_df.copy()\n",
    "    \n",
    "    def g(df):\n",
    "        df['conviction_quantile'] = pd.qcut(df['conviction'], quantiles, labels=False, duplicates=\"drop\")\n",
    "        df['top_quantile'] = pd.qcut(df['pred_2'], quantiles, labels=False, duplicates=\"drop\")\n",
    "        df['bottom_quantile'] = pd.qcut(df['pred_0'], quantiles, labels=False, duplicates=\"drop\")\n",
    "        return df\n",
    "        \n",
    "    results_df = results_df.groupby(\"date\").apply(g).reset_index(drop=True)\n",
    "    return results_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_time_period(results_df, first_date, last_date):\n",
    "    current_results = results_df.copy()\n",
    "    current_results = current_results[current_results[\"date\"] > pd.Timestamp(first_date)]\n",
    "    current_results = current_results[current_results[\"date\"] < pd.Timestamp(last_date)]\n",
    "    return current_results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba78f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results(df, exchange_codes = None, currencies = None, quantiles = 20, \n",
    "                    min_date = \"2020-01-01\", max_date=\"2023-12-31\", n_gvkeys = 500, svm=False,\n",
    "                    min_market_cap_percentile = 0.6,\n",
    "                   use_percentile_cap = False, min_volume_usd_5 = 1000, lower_rank = None, max_market_cap_percentile=None):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    if not (\"conviction\" in df.columns):\n",
    "        print(\"conviction not in columns\")\n",
    "        if svm:\n",
    "            df[\"conviction\"] = df[\"pred_1\"]\n",
    "        else:\n",
    "            df[\"conviction\"] = df[\"pred_2\"] - df[\"pred_0\"]\n",
    "        \n",
    "    \n",
    "    if exchange_codes != None:\n",
    "        df = df[df[\"exchange_code\"].isin(exchange_codes)]\n",
    "    if currencies != None:\n",
    "        df = df[df[\"currency\"].isin(currencies)]\n",
    "        \n",
    "        \n",
    "    df[\"trr_5_fwd_ar\"] = np.exp(df[\"trr_5_fwd\"]) - 1\n",
    "    if use_percentile_cap:\n",
    "        df = filter_market_caps(df, min_market_cap_percentile, max_market_cap_percentile)\n",
    "    else:\n",
    "        if \"market_cap_usd\" in df.columns:\n",
    "            df = df[df[\"volume_usd_5\"] >= min_volume_usd_5]\n",
    "            df[\"market_cap_rank\"] = df.groupby(\"date\")[\"market_cap_usd\"].rank(ascending=False, method=\"first\").astype(int)\n",
    "            df = df[df[\"market_cap_rank\"] <= n_gvkeys]\n",
    "            if lower_rank != None:\n",
    "                df = df[df[\"market_cap_rank\"] >= lower_rank]\n",
    "\n",
    "        elif \"market_cap_rank\" in df.columns:\n",
    "            df = df[df[\"volume_usd_5\"] >= min_volume_usd_5]\n",
    "            df = df[df[\"market_cap_rank\"] <= n_gvkeys]\n",
    "        else:\n",
    "            print(\"No market cap or rank in df\")\n",
    "    df = add_quantiles(df, quantiles=quantiles)\n",
    "    df = set_time_period(df, min_date, max_date)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc117d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dirs = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_lookup = pd.read_parquet(\"../data/lookup/us_lookup.parquet\", engine=\"pyarrow\")\n",
    "us_lookup[\"date\"] = pd.to_datetime(us_lookup[\"date\"])\n",
    "us_lookup.set_index([\"date\", \"gvkey\"], inplace=True)\n",
    "us_lookup.drop(columns=[\"currency\", \"country_hq\", \"exchange_code\", \"company_name\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df87c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cols = ['date', 'gvkey', 'company_name', 'currency', 'exchange_code',\n",
    "       'trr_5_fwd', 'trr_5_fwd_class', 'pred_0', 'pred_1', 'pred_2',\n",
    "       'pred_class', 'market_cap_rank', 'train_file', 'split_year', 'gsector',\n",
    "       'ggroup', 'gind', 'gsubind', 'market_cap_usd', 'trr_5', 'volume_usd_5', 'volatility_5',\n",
    "       'price_close_usd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5ceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble model, all predictions\n",
    "\n",
    "models = [\"catboost\",\"xgb\",\"logreg\",\"rf\"]\n",
    "regions = [\"US\", \"Europe\", \"Japan\"]\n",
    "\n",
    "no_conviction = False\n",
    "\n",
    "if no_conviction:\n",
    "    top_quantile_name = \"top_quantile\"\n",
    "    bottom_quantile_name = \"bottom_quantile\"\n",
    "    \n",
    "else:\n",
    "    top_quantile_name = \"conviction_quantile\"\n",
    "    bottom_quantile_name = \"conviction_quantile\"\n",
    "\n",
    "n_quantiles = 40\n",
    "\n",
    "mean_features = [\"trr_5_fwd_ar\", \"volume_usd_5\", \"volatility_5\"]\n",
    "\n",
    "max_trr_5_fwd = float(0.3) #Arithmetic = +100%\n",
    "min_trr_5_fwd = float(-0.3) #Arithmetic = -50%\n",
    "\n",
    "only_first_5_test_years = True\n",
    "\n",
    "keep_uuids = False\n",
    "\n",
    "years = [\n",
    "    #1980, 1985, 1990, 1995, \n",
    "    2000, 2005, 2010, 2015, 2020]\n",
    "\n",
    "result_dirs = os.listdir(\"results\")\n",
    "\n",
    "us_lookup = pd.read_parquet(\"data/lookup/us_lookup.parquet\", engine=\"pyarrow\")\n",
    "us_lookup[\"date\"] = pd.to_datetime(us_lookup[\"date\"])\n",
    "us_lookup.set_index([\"date\", \"gvkey\"], inplace=True)\n",
    "\n",
    "eu_lookup = pd.read_parquet(\"data/lookup/eu_lookup.parquet\", engine=\"pyarrow\")\n",
    "eu_lookup[\"date\"] = pd.to_datetime(eu_lookup[\"date\"])\n",
    "eu_lookup.set_index([\"date\", \"gvkey\"], inplace=True)\n",
    "\n",
    "jp_lookup = pd.read_parquet(\"data/lookup/jp_lookup.parquet\", engine=\"pyarrow\")\n",
    "jp_lookup[\"date\"] = pd.to_datetime(jp_lookup[\"date\"])\n",
    "jp_lookup.set_index([\"date\", \"gvkey\"], inplace=True)\n",
    "\n",
    "result_cols = ['date', 'gvkey', 'exchange_code',\n",
    "       'trr_5_fwd', 'trr_5_fwd_class', 'pred_0', 'pred_1', 'pred_2',\n",
    "        'train_file', 'split_year', 'gsector',\n",
    "       'ggroup', 'gind', 'gsubind', 'market_cap_usd', 'trr_5', 'volume_usd_5', 'volatility_5']\n",
    "\n",
    "train_files = ['return_filter_bear_m_short_2_3', 'bull_dates_sp500', 'markov_rec',\n",
    "           'flat_dates_sp500', 'nber_recession_dates',\n",
    "           'return_filter_bull_m_short_2_3', 'non_bear_dates_sp500',\n",
    "           'nber_expansion_dates', 'bear_dates_sp500',\n",
    "           'return_filter_bull_m_long_3_6_', 'all_dates',\n",
    "           'return_filter_bear_m_long_3_6_', 'markov_exp', \n",
    "            'EPU_exp_2yr','EPU_rec_2yr']\n",
    "\n",
    "mean_columns = [\"pred_0\", \"pred_1\", \"pred_2\"]\n",
    "\n",
    "for region in regions:\n",
    "    for y_i, year in enumerate(years):\n",
    "        first_results = True\n",
    "        if year < 2005:\n",
    "            if region == \"Europe\" or region == \"Japan\":\n",
    "                continue\n",
    "        print(\"year:\", year)\n",
    "        for train_file in train_files:\n",
    "            sys.stdout.write(f\"Processing {region} for {train_file} in {year}\\n\")\n",
    "            models_found = 0\n",
    "            for m_1, model_name in enumerate(models):\n",
    "                for directory in result_dirs:\n",
    "                    if model_name + \"_\" + region in directory and \"regime_feature\" not in directory and f\"test_split_{year}\" in directory and train_file in directory: \n",
    "                        if train_file == \"bear_dates_sp500\" and \"non_bear_dates_sp500\" in directory:\n",
    "                            continue\n",
    "                        sys.stdout.write(f\"Found {model_name} for {region} for {train_file} in {year}\\n\")\n",
    "                        models_found += 1\n",
    "                        sys.stdout.write(\"Models found: \" + str(models_found) + \"\\n\")\n",
    "                        current_single_model_results = pd.read_parquet(f\"results/{directory}/results.parquet\", engine=\"pyarrow\")\n",
    "                        current_single_model_results[\"date\"] = pd.to_datetime(current_single_model_results[\"date\"])\n",
    "                        current_single_model_results = current_single_model_results[current_single_model_results[\"date\"] <= (pd.Timestamp(f\"{year}-01-01\") + pd.DateOffset(years=5))]\n",
    "                        if m_1 == 0:\n",
    "                            ensemble_results = current_single_model_results.copy()\n",
    "                            continue\n",
    "                        ensemble_results = pd.concat([ensemble_results, current_single_model_results])\n",
    "            if models_found == len(models):\n",
    "                current_results = ensemble_results.groupby([\"date\", \"gvkey\"])[mean_columns + [\"trr_5_fwd\", \"trr_5_fwd_class\"]].mean().reset_index()\n",
    "                \n",
    "                current_results[\"train_file\"] = train_file\n",
    "                current_results[\"split_year\"] = year\n",
    "\n",
    "                if \"model\" in current_results.columns:\n",
    "                    current_results.drop(columns=[\"model\"], inplace=True)\n",
    "\n",
    "                if only_first_5_test_years:\n",
    "                    current_results = current_results[current_results[\"date\"] < (pd.Timestamp(f\"{year}-01-01\") + pd.DateOffset(years=5))]\n",
    "                    \n",
    "                current_results = current_results[current_results[\"date\"] >= (pd.Timestamp(f\"{year}-01-01\"))]\n",
    "                    \n",
    "                if region == \"US\":\n",
    "                    current_results = current_results.set_index([\"date\", \"gvkey\"]).merge(us_lookup, left_index=True, right_index=True, suffixes=(\"_x\", \"\")).reset_index()\n",
    "                elif region == \"Europe\":\n",
    "                    current_results = current_results.set_index([\"date\", \"gvkey\"]).merge(eu_lookup, left_index=True, right_index=True, suffixes=(\"_x\", \"\")).reset_index()\n",
    "                elif region == \"Japan\":\n",
    "                    current_results = current_results.set_index([\"date\", \"gvkey\"]).merge(jp_lookup, left_index=True, right_index=True, suffixes=(\"_x\", \"\")).reset_index()\n",
    "                    \n",
    "                if max_trr_5_fwd:\n",
    "                    current_results = current_results[current_results[\"trr_5_fwd\"] <= max_trr_5_fwd]\n",
    "                    \n",
    "                if min_trr_5_fwd:\n",
    "                    current_results = current_results[current_results[\"trr_5_fwd\"] >= min_trr_5_fwd]\n",
    "\n",
    "                current_results = current_results[result_cols]\n",
    "\n",
    "                current_results_copy = current_results.copy()\n",
    "\n",
    "                min_date = f\"{year}-01-01\"\n",
    "                \n",
    "                \n",
    "                current_results = prepare_results(current_results, quantiles=n_quantiles, min_date = min_date, \n",
    "                                                    use_percentile_cap=True, min_market_cap_percentile = 0.78, \n",
    "                                                    max_market_cap_percentile=1.00)\n",
    "\n",
    "                current_results[\"train_file\"] = train_file\n",
    "                if first_results:\n",
    "                    all_train_results = current_results\n",
    "                    first_results = False\n",
    "                    continue\n",
    "                all_train_results = pd.concat([all_train_results, current_results])\n",
    "            else:\n",
    "                sys.stdout.write(f\"Not enough models found for {region} for {train_file} in {year}\\n\")\n",
    "                continue\n",
    "                \n",
    "        all_train_results[\"split_year\"] = year\n",
    "        if ((y_i == 0) and (region == \"US\")):\n",
    "            all_year_results = all_train_results\n",
    "            continue\n",
    "        if ((year == 2005) and (region == \"Europe\")) or ((year == 2005) and (region == \"Japan\")):\n",
    "            all_year_results = all_train_results\n",
    "            continue\n",
    "        all_year_results = pd.concat([all_year_results, all_train_results])\n",
    "        \n",
    "    model_name_and_region = f\"{models}_ensemble_{region}\"\n",
    "        \n",
    "    if max_trr_5_fwd:\n",
    "        model_name_and_region += f\"max_trr_5_fwd_ar_{max_trr_5_fwd}\"\n",
    "                    \n",
    "    if min_trr_5_fwd:\n",
    "        model_name_and_region += f\"min_trr_5_fwd_ar_{min_trr_5_fwd}\"\n",
    "\n",
    "    all_year_results[\"model\"] = model_name_and_region\n",
    "    all_year_results.to_parquet(f\"results/digests/{model_name_and_region}_all_preds_078_after_2003.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
