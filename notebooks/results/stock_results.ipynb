{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67214d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "import matplotlib.ticker as mtick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ece8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = \"trr_5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb31510",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda10997",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e568c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_market_caps(results_df, min_market_cap_percentile = 0.6, max_market_cap_percentile = None):\n",
    "    current_results = results_df.copy()\n",
    "    \n",
    "    min_market_caps = current_results.groupby(\"date\")[\"market_cap_usd\"].quantile(min_market_cap_percentile)\n",
    "    \n",
    "    if max_market_cap_percentile != None:\n",
    "        max_market_caps = current_results.groupby(\"date\")[\"market_cap_usd\"].quantile(max_market_cap_percentile)\n",
    "           \n",
    "    current_results = current_results.groupby(\"date\").apply(lambda x: x[x[\"market_cap_usd\"] >= min_market_caps.loc[x.name]]).reset_index(drop=True)\n",
    "\n",
    "    if max_market_cap_percentile != None:\n",
    "        current_results = current_results.groupby(\"date\").apply(lambda x: x[x[\"market_cap_usd\"] <= max_market_caps.loc[x.name]]).reset_index(drop=True)\n",
    "    current_results.sort_values([\"date\", \"gvkey\"], inplace=True)\n",
    "\n",
    "    return current_results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f856f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_quantiles(results_df, quantiles=10):\n",
    "    results_df = results_df.copy()\n",
    "    \n",
    "    def g(df):\n",
    "        df['conviction_quantile'] = pd.qcut(df['conviction'], quantiles, labels=False, duplicates=\"drop\")\n",
    "        df['top_quantile'] = pd.qcut(df['pred_2'], quantiles, labels=False, duplicates=\"drop\")\n",
    "        df['bottom_quantile'] = pd.qcut(df['pred_0'], quantiles, labels=False, duplicates=\"drop\")\n",
    "        return df\n",
    "        \n",
    "    results_df = results_df.groupby(\"date\").apply(g).reset_index(drop=True)\n",
    "    return results_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a916dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_time_period(results_df, first_date, last_date):\n",
    "    current_results = results_df.copy()\n",
    "    current_results = current_results[current_results[\"date\"] > pd.Timestamp(first_date)]\n",
    "    current_results = current_results[current_results[\"date\"] < pd.Timestamp(last_date)]\n",
    "    return current_results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde69947",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_market_cap_percentile_us = 0.6\n",
    "min_market_cap_percentile_global = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe3d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results(df, exchange_codes = None, currencies = None, quantiles = 20, \n",
    "                    min_date = \"2020-01-01\", max_date=\"2023-12-31\", n_gvkeys = 500, svm=False,\n",
    "                    min_market_cap_percentile = 0.6,\n",
    "                   use_percentile_cap = False, min_volume_usd_5 = 1000, lower_rank = None, max_market_cap_percentile=None):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    if not (\"conviction\" in df.columns):\n",
    "        print(\"conviction not in columns\")\n",
    "        if svm:\n",
    "            df[\"conviction\"] = df[\"pred_1\"]\n",
    "        else:\n",
    "            df[\"conviction\"] = df[\"pred_2\"] - df[\"pred_0\"]\n",
    "        \n",
    "    \n",
    "    if exchange_codes != None:\n",
    "        df = df[df[\"exchange_code\"].isin(exchange_codes)]\n",
    "    if currencies != None:\n",
    "        df = df[df[\"currency\"].isin(currencies)]\n",
    "        \n",
    "        \n",
    "    df[\"trr_5_fwd_ar\"] = np.exp(df[\"trr_5_fwd\"]) - 1\n",
    "    if use_percentile_cap:\n",
    "        df = filter_market_caps(df, min_market_cap_percentile, max_market_cap_percentile)\n",
    "    else:\n",
    "        if \"market_cap_usd\" in df.columns:\n",
    "            df = df[df[\"volume_usd_5\"] >= min_volume_usd_5]\n",
    "            df[\"market_cap_rank\"] = df.groupby(\"date\")[\"market_cap_usd\"].rank(ascending=False, method=\"first\").astype(int)\n",
    "            df = df[df[\"market_cap_rank\"] <= n_gvkeys]\n",
    "            if lower_rank != None:\n",
    "                df = df[df[\"market_cap_rank\"] >= lower_rank]\n",
    "\n",
    "        elif \"market_cap_rank\" in df.columns:\n",
    "            df = df[df[\"volume_usd_5\"] >= min_volume_usd_5]\n",
    "            df = df[df[\"market_cap_rank\"] <= n_gvkeys]\n",
    "        else:\n",
    "            print(\"No market cap or rank in df\")\n",
    "    df = add_quantiles(df, quantiles=quantiles)\n",
    "    df = set_time_period(df, min_date, max_date)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_lookup = pd.read_parquet(\"../data/lookup/us_lookup.parquet\", engine=\"pyarrow\")\n",
    "us_lookup[\"date\"] = pd.to_datetime(us_lookup[\"date\"])\n",
    "us_lookup[\"trr_5_ar\"] = np.exp(us_lookup[\"trr_5\"]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9083fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_lookup = pd.read_parquet(\"../data/lookup/eu_lookup.parquet\", engine=\"pyarrow\")\n",
    "eu_lookup[\"date\"] = pd.to_datetime(eu_lookup[\"date\"])\n",
    "eu_lookup[\"trr_5_ar\"] = np.exp(eu_lookup[\"trr_5\"]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ab05e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_lookup = pd.read_parquet(\"../data/lookup/jp_lookup.parquet\", engine=\"pyarrow\")\n",
    "jp_lookup[\"date\"] = pd.to_datetime(jp_lookup[\"date\"])\n",
    "jp_lookup[\"trr_5_ar\"] = np.exp(jp_lookup[\"trr_5\"]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cfa30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cols = ['date', 'gvkey', 'company_name', 'currency', 'exchange_code',\n",
    "       'trr_5_fwd', 'trr_5_fwd_class', 'pred_0', 'pred_1', 'pred_2',\n",
    "       'pred_class', 'market_cap_rank', 'train_file', 'split_year', 'gsector',\n",
    "       'ggroup', 'gind', 'gsubind', 'market_cap_usd', 'trr_5', 'volume_usd_5', 'volatility_5',\n",
    "       'price_close_usd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d33001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_digest(path, region_name, model_name, keep_groups = True):\n",
    "    digest = pd.read_parquet(path, engine=\"pyarrow\")\n",
    "    digest[\"date\"] = pd.to_datetime(digest[\"date\"])\n",
    "    digest[\"region\"] = region_name\n",
    "    digest[\"model\"] = model_name\n",
    "    digest.reset_index(inplace=True, drop=True)\n",
    "    digest[\"min_max_quantile\"] = list(zip(digest[\"min_quantile\"], digest[\"max_quantile\"]))\n",
    "    if not keep_groups:\n",
    "        digest = digest[digest[\"all\"] == True]\n",
    "    return digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_part_files = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c16a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ensemble_us_digest = load_digest(\"digests/['catboost', 'xgb', 'logreg', 'rf']_ensemble_USmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\", \n",
    "                                         \"us\", \"all_ensemble\")\n",
    "all_ensemble_eu_digest = load_digest(\"digests/['catboost', 'xgb', 'logreg', 'rf']_ensemble_Europemax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\", \n",
    "                                         \"eu\", \"all_ensemble\")\n",
    "all_ensemble_jp_digest = load_digest(\"digests/['catboost', 'xgb', 'logreg', 'rf']_ensemble_Japanmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\", \n",
    "                                         \"jp\", \"all_ensemble\")\n",
    "all_ensemble_digest = pd.concat([all_ensemble_us_digest, all_ensemble_eu_digest, all_ensemble_jp_digest])\n",
    "\n",
    "if delete_part_files:\n",
    "    del all_ensemble_us_digest, all_ensemble_eu_digest, all_ensemble_jp_digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "brf_ensemble_us_digest = load_digest(\"digests/['catboost', 'xgb', 'rf']_ensemble_USmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                         \"us\", \"brf_ensemble\")\n",
    "brf_ensemble_eu_digest = load_digest(\"digests/['catboost', 'xgb', 'rf']_ensemble_Europemax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                         \"eu\", \"brf_ensemble\")\n",
    "brf_ensemble_jp_digest = load_digest(\"digests/['catboost', 'xgb', 'rf']_ensemble_Japanmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                         \"jp\", \"brf_ensemble\")\n",
    "brf_ensemble_digest = pd.concat([brf_ensemble_us_digest, brf_ensemble_eu_digest, brf_ensemble_jp_digest])\n",
    "\n",
    "if delete_part_files:\n",
    "    del brf_ensemble_us_digest, brf_ensemble_eu_digest, brf_ensemble_jp_digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ensemble_us_digest = load_digest(\"digests/['catboost', 'xgb']_ensemble_USmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                       \"us\", \"b_ensemble\")\n",
    "b_ensemble_eu_digest = load_digest(\"digests/['catboost', 'xgb']_ensemble_Europemax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                       \"eu\", \"b_ensemble\")\n",
    "b_ensemble_jp_digest = load_digest(\"digests/['catboost', 'xgb']_ensemble_Japanmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                       \"jp\", \"b_ensemble\")\n",
    "b_ensemble_digest = pd.concat([b_ensemble_us_digest, b_ensemble_eu_digest, b_ensemble_jp_digest])\n",
    "\n",
    "if delete_part_files:\n",
    "    del b_ensemble_us_digest, b_ensemble_eu_digest, b_ensemble_jp_digest\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b0720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_ensemble_us_digest = load_digest(\"digests/['catboost', 'rf']_ensemble_USmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                       \"us\", \"crf_ensemble\")\n",
    "crf_ensemble_eu_digest = load_digest(\"digests/['catboost', 'rf']_ensemble_Europemax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                       \"eu\", \"crf_ensemble\")\n",
    "crf_ensemble_jp_digest = load_digest(\"digests/['catboost', 'rf']_ensemble_Japanmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                       \"jp\", \"crf_ensemble\")\n",
    "crf_ensemble_digest = pd.concat([crf_ensemble_us_digest, crf_ensemble_eu_digest, crf_ensemble_jp_digest])\n",
    "\n",
    "if delete_part_files:\n",
    "    del crf_ensemble_us_digest, crf_ensemble_eu_digest, crf_ensemble_jp_digest\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_digests = pd.concat([all_ensemble_digest, brf_ensemble_digest, b_ensemble_digest, crf_ensemble_digest])\n",
    "\n",
    "if delete_part_files:\n",
    "    del all_ensemble_digest, brf_ensemble_digest, b_ensemble_digest, crf_ensemble_digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a97dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_us_digest = load_digest(\"digests/catboost_USmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"us\", \"catboost\")\n",
    "catboost_eu_digest = load_digest(\"digests/catboost_Europemax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"eu\", \"catboost\")\n",
    "catboost_jp_digest = load_digest(\"digests/catboost_Japanmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"jp\", \"catboost\")\n",
    "catboost_digest = pd.concat([catboost_us_digest, catboost_eu_digest, catboost_jp_digest])\n",
    "\n",
    "if delete_part_files:\n",
    "    del catboost_us_digest, catboost_eu_digest, catboost_jp_digest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_us_digest = load_digest(\"digests/xgb_USmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"us\", \"xgb\")\n",
    "xgb_eu_digest = load_digest(\"digests/xgb_Europemax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"eu\", \"xgb\")\n",
    "xgb_jp_digest = load_digest(\"digests/xgb_Japanmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"jp\", \"xgb\")\n",
    "xgb_digest = pd.concat([xgb_us_digest, xgb_eu_digest, xgb_jp_digest])\n",
    "\n",
    "if delete_part_files:\n",
    "    del xgb_us_digest, xgb_eu_digest, xgb_jp_digest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74fcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_us_digest = load_digest(\"digests/logreg_USmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"us\", \"logreg\")\n",
    "logreg_eu_digest = load_digest(\"digests/logreg_Europemax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"eu\", \"logreg\")\n",
    "logreg_jp_digest = load_digest(\"digests/logreg_Japanmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"jp\", \"logreg\")\n",
    "logreg_digest = pd.concat([logreg_us_digest, logreg_eu_digest, logreg_jp_digest])\n",
    "\n",
    "if delete_part_files:\n",
    "    del logreg_us_digest, logreg_eu_digest, logreg_jp_digest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1737cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_us_digest = load_digest(\"digests/rf_USmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"us\", \"rf\")\n",
    "rf_eu_digest = load_digest(\"digests/rf_Europemax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"eu\", \"rf\")\n",
    "rf_jp_digest = load_digest(\"digests/rf_Japanmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"jp\", \"rf\")\n",
    "rf_digest = pd.concat([rf_us_digest, rf_eu_digest, rf_jp_digest])\n",
    "\n",
    "if delete_part_files:\n",
    "    del rf_us_digest, rf_eu_digest, rf_jp_digest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_us_digest = load_digest(\"digests/svm_USmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"us\", \"svm\")\n",
    "svm_eu_digest = load_digest(\"digests/svm_Europemax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"eu\", \"svm\")\n",
    "svm_jp_digest = load_digest(\"digests/svm_Japanmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_digest.parquet\",\n",
    "                                 \"jp\", \"svm\")\n",
    "svm_digest = pd.concat([svm_us_digest, svm_eu_digest, svm_jp_digest])\n",
    "\n",
    "if delete_part_files:\n",
    "    del svm_us_digest, svm_eu_digest, svm_jp_digest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92216d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_digests = pd.concat([ensemble_digests, catboost_digest, xgb_digest, logreg_digest, rf_digest, svm_digest])\n",
    "\n",
    "if delete_part_files:\n",
    "    del ensemble_digests, catboost_digest, xgb_digest, logreg_digest, rf_digest, svm_digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3822a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_digests = all_ensemble_digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33987e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_digests_nogroup = all_digests[all_digests[\"all\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac85f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_digests_nogroup = all_digests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_lookup = pd.read_parquet(\"../data/lookup/us_lookup.parquet\", engine=\"pyarrow\")\n",
    "us_lookup[\"date\"] = pd.to_datetime(us_lookup[\"date\"])\n",
    "\n",
    "us_lookup_mc_cap = filter_market_caps(us_lookup, 0.78, 1)\n",
    "us_lookup_mc_cap = us_lookup_mc_cap[us_lookup_mc_cap[\"trr_5\"] <= 0.3]\n",
    "us_lookup_mc_cap = us_lookup_mc_cap[us_lookup_mc_cap[\"trr_5\"] >= -0.3]\n",
    "us_lookup_mc_cap = us_lookup_mc_cap[us_lookup_mc_cap[\"date\"].dt.dayofweek == 4]\n",
    "us_lookup_mc_cap_avg = us_lookup_mc_cap.groupby(\"date\")[\"trr_5_ar\"].mean()\n",
    "\n",
    "eu_lookup = pd.read_parquet(\"../data/lookup/eu_lookup.parquet\", engine=\"pyarrow\")\n",
    "eu_lookup[\"date\"] = pd.to_datetime(eu_lookup[\"date\"])\n",
    "\n",
    "eu_lookup_mc_cap = filter_market_caps(eu_lookup, 0.78, 1)\n",
    "eu_lookup_mc_cap = eu_lookup_mc_cap[eu_lookup_mc_cap[\"trr_5\"] <= 0.3]\n",
    "eu_lookup_mc_cap = eu_lookup_mc_cap[eu_lookup_mc_cap[\"trr_5\"] >= -0.3]\n",
    "eu_lookup_mc_cap = eu_lookup_mc_cap[eu_lookup_mc_cap[\"date\"].dt.dayofweek == 4]\n",
    "eu_lookup_mc_cap_avg = eu_lookup_mc_cap.groupby(\"date\")[\"trr_5_ar\"].mean()\n",
    "\n",
    "jp_lookup = pd.read_parquet(\"../data/lookup/jp_lookup.parquet\", engine=\"pyarrow\")\n",
    "jp_lookup[\"date\"] = pd.to_datetime(jp_lookup[\"date\"])\n",
    "\n",
    "jp_lookup_mc_cap = filter_market_caps(jp_lookup, 0.78, 1)\n",
    "jp_lookup_mc_cap = jp_lookup_mc_cap[jp_lookup_mc_cap[\"trr_5\"] <= 0.3]\n",
    "jp_lookup_mc_cap = jp_lookup_mc_cap[jp_lookup_mc_cap[\"trr_5\"] >= -0.3]\n",
    "jp_lookup_mc_cap = jp_lookup_mc_cap[jp_lookup_mc_cap[\"date\"].dt.dayofweek == 4]\n",
    "jp_lookup_mc_cap_avg = jp_lookup_mc_cap.groupby(\"date\")[\"trr_5_ar\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb5fd9d",
   "metadata": {},
   "source": [
    "# Historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa613e65",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "current_min_max_quantile = (0.78,1)\n",
    "\n",
    "plot_feature = \"top_minus_bottom_4_trr_5_fwd_ar_mean\"\n",
    "\n",
    "resample_freq = \"YS\"\n",
    "\n",
    "current_digest = all_digests_nogroup[(all_digests_nogroup[\"region\"] == \"us\") & \n",
    "                                     (all_digests_nogroup[\"min_max_quantile\"] == current_min_max_quantile) &\n",
    "                                    (all_digests_nogroup[\"train_file\"] == \"all_dates\")]\n",
    "\n",
    "\n",
    "avg_trr_5_ar = us_lookup_mc_cap[us_lookup_mc_cap[\"date\"] > pd.Timestamp(\"1980-01-01\")].groupby(\"date\")[\"trr_5_ar\"].mean().reset_index()\n",
    "avg_trr_5_ar.set_index(\"date\")[\"trr_5_ar\"].resample(resample_freq).mean().plot(ax=ax, label=\"Average Return\")\n",
    "\n",
    "current_digest[current_digest[\"model\"] == \"logreg\"].set_index(\"date\")[plot_feature].resample(resample_freq).mean().plot(ax=ax, label=\"Logistic Regression\")\n",
    "current_digest[current_digest[\"model\"] == \"catboost\"].set_index(\"date\")[plot_feature].resample(resample_freq).mean().plot(ax=ax, label=\"CatBoost\")\n",
    "current_digest[current_digest[\"model\"] == \"xgb\"].set_index(\"date\")[plot_feature].resample(resample_freq).mean().plot(ax=ax, label=\"XGBoost\")\n",
    "current_digest[current_digest[\"model\"] == \"rf\"].set_index(\"date\")[plot_feature].resample(resample_freq).mean().plot(ax=ax, label=\"Random Forest\")\n",
    "current_digest[current_digest[\"model\"] == \"svm\"].set_index(\"date\")[plot_feature].resample(resample_freq).mean().plot(ax=ax, label=\"Support Vector Machine\")\n",
    "current_digest[current_digest[\"model\"] == \"all_ensemble\"].set_index(\"date\")[plot_feature].resample(resample_freq).mean().plot(ax=ax, label=\"Ensemble (ex. SVM)\")\n",
    "\n",
    "ax.axvline(pd.Timestamp(\"2003-01-01\"), color=\"black\", linestyle=\"--\", lw=2)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "ax.legend(fontsize=16)\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"\")\n",
    "plt.tight_layout()\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/all_models_avg_returns_all_trained_min078.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d23cf52",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f80330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12,8))\n",
    "\n",
    "min_date = pd.Timestamp(\"2003-01-01\")\n",
    "max_date = pd.Timestamp(\"2023-12-31\")\n",
    "\n",
    "\n",
    "current_min_max_quantile = (0.78,1)\n",
    "\n",
    "current_digest = all_digests_nogroup[(all_digests_nogroup[\"region\"] == \"us\") & \n",
    "                                     (all_digests_nogroup[\"min_max_quantile\"] == current_min_max_quantile) &\n",
    "                                    (all_digests_nogroup[\"train_file\"] == \"all_dates\")]\n",
    "\n",
    "current_digest = current_digest[(current_digest[\"date\"] > min_date) & (current_digest[\"date\"] < max_date)]\n",
    "\n",
    "\n",
    "top_n_quantiles = 4\n",
    "\n",
    "current_digest_model = current_digest[current_digest[\"model\"] == \"logreg\"]\n",
    "\n",
    "i = 0\n",
    "bar_width=0.20\n",
    "\n",
    "bar_all = ax.bar(height = current_digest_model[f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i-0.2, width=bar_width, label=\"Total\", color=\"tab:purple\", edgecolor=\"black\")\n",
    "bar_longs = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i, width=bar_width, label=\"Longs\", color=\"tab:blue\", edgecolor=\"black\")\n",
    "bar_shorts = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i+0.2, width=bar_width, label=\"Shorts\", color=\"tab:orange\", edgecolor=\"black\")\n",
    "\n",
    "i+= 1\n",
    "\n",
    "current_digest_model = current_digest[current_digest[\"model\"] == \"catboost\"]\n",
    "\n",
    "\n",
    "bar_all = ax.bar(height = current_digest_model[f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i-0.2, width=bar_width, color=\"tab:purple\", edgecolor=\"black\")\n",
    "bar_longs = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i, width=bar_width,  color=\"tab:blue\", edgecolor=\"black\")\n",
    "bar_shorts = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i+0.2, width=bar_width, color=\"tab:orange\", edgecolor=\"black\")\n",
    "\n",
    "i+= 1\n",
    "\n",
    "current_digest_model = current_digest[current_digest[\"model\"] == \"xgb\"]\n",
    "\n",
    "\n",
    "bar_all = ax.bar(height = current_digest_model[f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i-0.2, width=bar_width, color=\"tab:purple\", edgecolor=\"black\")\n",
    "bar_longs = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i, width=bar_width, color=\"tab:blue\", edgecolor=\"black\")\n",
    "bar_shorts = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i+0.2, width=bar_width, color=\"tab:orange\", edgecolor=\"black\")\n",
    "\n",
    "i+= 1\n",
    "\n",
    "current_digest_model = current_digest[current_digest[\"model\"] == \"rf\"]\n",
    "\n",
    "bar_all = ax.bar(height = current_digest_model[f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i-0.2, width=bar_width, color=\"tab:purple\", edgecolor=\"black\")\n",
    "bar_longs = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i, width=bar_width, color=\"tab:blue\", edgecolor=\"black\")\n",
    "bar_shorts = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i+0.2, width=bar_width, color=\"tab:orange\", edgecolor=\"black\")\n",
    "\n",
    "i+= 1\n",
    "\n",
    "current_digest_model = current_digest[current_digest[\"model\"] == \"svm\"]\n",
    "\n",
    "\n",
    "bar_all = ax.bar(height = current_digest_model[f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i-0.2, width=bar_width, color=\"tab:purple\", edgecolor=\"black\")\n",
    "bar_longs = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i, width=bar_width, color=\"tab:blue\", edgecolor=\"black\")\n",
    "bar_shorts = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i+0.2, width=bar_width, color=\"tab:orange\", edgecolor=\"black\")\n",
    "\n",
    "i+= 1\n",
    "\n",
    "current_digest_model = current_digest[current_digest[\"model\"] == \"all_ensemble\"]\n",
    "\n",
    "bar_all = ax.bar(height = current_digest_model[f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i-0.2, width=bar_width,  color=\"tab:purple\", edgecolor=\"black\")\n",
    "bar_longs = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i, width=bar_width, color=\"tab:blue\", edgecolor=\"black\")\n",
    "bar_shorts = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i+0.2, width=bar_width, color=\"tab:orange\", edgecolor=\"black\")\n",
    "\n",
    "i+= 1\n",
    "\n",
    "current_digest_model = current_digest[current_digest[\"model\"] == \"brf_ensemble\"]\n",
    "\n",
    "bar_all = ax.bar(height = current_digest_model[f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i-0.2, width=bar_width,  color=\"tab:purple\", edgecolor=\"black\")\n",
    "bar_longs = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i, width=bar_width, color=\"tab:blue\", edgecolor=\"black\")\n",
    "bar_shorts = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i+0.2, width=bar_width, color=\"tab:orange\", edgecolor=\"black\")\n",
    "\n",
    "i+= 1\n",
    "\n",
    "current_digest_model = current_digest[current_digest[\"model\"] == \"b_ensemble\"]\n",
    "\n",
    "bar_all = ax.bar(height = current_digest_model[f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i-0.2, width=bar_width,  color=\"tab:purple\", edgecolor=\"black\")\n",
    "bar_longs = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i, width=bar_width, color=\"tab:blue\", edgecolor=\"black\")\n",
    "bar_shorts = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i+0.2, width=bar_width, color=\"tab:orange\", edgecolor=\"black\")\n",
    "\n",
    "i+= 1\n",
    "\n",
    "current_digest_model = current_digest[current_digest[\"model\"] == \"crf_ensemble\"]\n",
    "\n",
    "bar_all = ax.bar(height = current_digest_model[f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i-0.2, width=bar_width,  color=\"tab:purple\", edgecolor=\"black\")\n",
    "bar_longs = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i, width=bar_width, color=\"tab:blue\", edgecolor=\"black\")\n",
    "bar_shorts = ax.bar(height = current_digest_model[current_digest_model[\"date\"].between(min_date, max_date)][f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "   x=i+0.2, width=bar_width, color=\"tab:orange\", edgecolor=\"black\")\n",
    "\n",
    "i+= 1\n",
    "\n",
    "avg_trr_5_ar = us_lookup_mc_cap[us_lookup_mc_cap[\"date\"].dt.dayofweek == 4]\n",
    "avg_trr_5_ar = avg_trr_5_ar[(avg_trr_5_ar[\"date\"] >= min_date) & (avg_trr_5_ar[\"date\"] <= max_date)].groupby(\"date\")[\"trr_5_ar\"].mean().mean()\n",
    "ax.axhline(avg_trr_5_ar, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "ax.axhline(0, color=\"black\")\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "ax.tick_params(axis='both', which='major', labelsize=18, labelbottom=True)\n",
    "ax.set_xticks(range(9))\n",
    "ax.set_xticklabels([\"Logistic Regression\", \"CatBoost\", \"XGBoost\", \"Random Forest\", \"Support Vector Machine\", \"Ensemble (ex. SVM)\", \"Boosting + RF Ensemble\", \"Boosting Ensemble\", \"CatBoost + RF Ensemble\"], rotation=45, ha='right')\n",
    "ax.set_ylim(top=0.004)\n",
    "ax.grid(axis=\"y\")\n",
    "ax.legend(fontsize=15)\n",
    "plt.tight_layout()\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/all_models_returns_bar_after_2003_min078_top10pct.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94adbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/all_models_returns_bar_after_2003_min078_top2p5pct.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b3f843",
   "metadata": {},
   "source": [
    "# Market cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e5558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_ranges = [(0, 0.12), (0.12, 0.34), (0.34, 0.56), (0.56, 0.78), (0.78, 0.89), (0.89,  1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede46c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "quantile_returns = all_ensemble_us_digest_cap[all_ensemble_us_digest_cap[\"min_max_quantile\"].isin(quantile_ranges)].groupby([\"min_max_quantile\", \"date\"])[[\"top_minus_bottom_4_trr_5_fwd_ar_mean\", \"n_stocks_in_group\"]].agg({\"top_minus_bottom_4_trr_5_fwd_ar_mean\" : \"mean\", \"n_stocks_in_group\" : \"median\"}).reset_index()\n",
    "\n",
    "for quantile_range in quantile_ranges:\n",
    "    quantile_returns[quantile_returns[\"min_max_quantile\"] == quantile_range].plot(x=\"date\", y=\"top_minus_bottom_4_trr_5_fwd_ar_mean\", ax=ax, label=str(quantile_range))\n",
    "\n",
    "n_stocks_in_group = quantile_returns[quantile_returns.index.isin(quantile_ranges)][\"n_stocks_in_group\"]\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.set_xlabel(\"Market Cap Quantile Ranges\", fontsize = 16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17452169",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, quantile_range in enumerate(quantile_ranges):\n",
    "    print(i, quantile_range)\n",
    "    current_cap = us_lookup_mc_cap\n",
    "    current_cap = current_cap.groupby(\"date\")[[\"trr_5\", \"trr_5_ar\", \"volume_usd_5\", \"volatility_5\", \"market_cap_usd\"]].mean().reset_index()\n",
    "    current_cap[\"min_max_quantile\"] = str(quantile_range)\n",
    "    if i == 0:\n",
    "        cap_means = current_cap\n",
    "    else:\n",
    "        cap_means = pd.concat([cap_means, current_cap])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_means[\"volatility_5_adj_annual\"] = cap_means[\"volatility_5\"] * np.sqrt(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8b4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOLATILITY OF STOCKS THEMSELVES:\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "\n",
    "for quantile_range in quantile_ranges:\n",
    "    cap_means[cap_means[\"min_max_quantile\"] == str(quantile_range)][[\"date\", \"volatility_5_adj_annual\"]].rolling(f\"{60}D\", on=\"date\").mean().plot(x=\"date\", y=\"volatility_5_adj_annual\", ax=ax, label=str(quantile_range), lw=2)\n",
    "\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "ax.set_ylabel(\"5-day Volatilitity (Annualised)\", fontsize = 18)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.legend(fontsize=16)\n",
    "plt.tight_layout()\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2faa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/volatility_market_cap_quantile_ranges_cap_rmean_60D_historical_annualised.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d18233",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 8))\n",
    "current_digest = all_digests_nogroup[(all_digests_nogroup[\"train_file\"] == \"all_dates\") &\n",
    "                                     (all_digests_nogroup[\"model\"] == \"all_ensemble\")]\n",
    "\n",
    "current_digest = current_digest[current_digest[\"region\"] == \"us\"]\n",
    "\n",
    "min_date = pd.Timestamp(\"2003-01-01\")\n",
    "\n",
    "current_digest = current_digest[current_digest[\"date\"] >= min_date]\n",
    "\n",
    "top_n_quantiles = 4\n",
    "\n",
    "grouped_df = current_digest[current_digest[\"min_max_quantile\"].isin(quantile_ranges)].groupby([\"min_quantile\", \"max_quantile\"])[[f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\", f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\", f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\", \"n_stocks_in_group\"]].mean()\n",
    "\n",
    "n_stocks_in_group = grouped_df[grouped_df.index.isin(quantile_ranges)][\"n_stocks_in_group\"]\n",
    "\n",
    "bar_width = 0.2\n",
    "hatch=None\n",
    "\n",
    "for i, train_file in enumerate(grouped_df.index):\n",
    "    if i == 0:\n",
    "        bar_all = ax.bar(height = grouped_df[grouped_df.index == train_file][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"], \n",
    "           x=i-0.2, width=bar_width, label=\"Total\", color=\"tab:purple\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_longs = ax.bar(height = grouped_df[grouped_df.index == train_file][f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\"], \n",
    "           x=i, width=bar_width, label=\"Longs\", color=\"tab:blue\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_shorts = ax.bar(height = grouped_df[grouped_df.index == train_file][f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"],\n",
    "           x=i+0.2, width=bar_width, label=\"Shorts\", color=\"tab:orange\", edgecolor=\"black\", hatch=hatch)\n",
    "    else:\n",
    "        bar_all = ax.bar(height = grouped_df[grouped_df.index == train_file][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"], \n",
    "           x=i-0.2, width=bar_width, color=\"tab:purple\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_longs = ax.bar(height = grouped_df[grouped_df.index == train_file][f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\"], \n",
    "           x=i, width=bar_width, color=\"tab:blue\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_shorts = ax.bar(height = grouped_df[grouped_df.index == train_file][f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"],\n",
    "           x=i+0.2, width=bar_width, color=\"tab:orange\", edgecolor=\"black\", hatch=hatch)\n",
    "    \n",
    "    \n",
    "    text_height = max(bar_longs.patches[0].get_height(), bar_all.patches[0].get_height())\n",
    "\n",
    "    ax.annotate(text = f\"n = {int(n_stocks_in_group.iloc[i])}\", \n",
    "           xy = (bar_longs.patches[0].get_x() + bar_longs.patches[0].get_width() / 2, \n",
    "            text_height + 0.0012), ha='center', va='center',\n",
    "           size=16\n",
    "           )\n",
    "    mean_return = cap_means[(cap_means[\"date\"] >= min_date) & (cap_means[\"min_max_quantile\"] == str(quantile_ranges[i]))][\"trr_5\"].mean()\n",
    "\n",
    "    ax.hlines(mean_return,bar_longs.patches[0].get_x() - bar_width, bar_longs.patches[0].get_x() + 2*bar_width, color=\"black\", linestyle=\"--\", lw=2)\n",
    "\n",
    "    \n",
    "n_stocks_in_group = quantile_returns[quantile_returns.index.isin(quantile_ranges)][\"n_stocks_in_group\"]\n",
    "\n",
    "    \n",
    "    \n",
    "ax.axhline(0, color=\"black\")\n",
    "    \n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "ax.set_xlabel(\"Market Cap Quantile Ranges\", fontsize = 20)\n",
    "ax.legend(fontsize=16)\n",
    "ax.set_xticks(range(len(quantile_ranges)))\n",
    "ax.set_xticklabels(quantile_ranges)\n",
    "plt.tight_layout()\n",
    "ax.grid(axis=\"y\")\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b858e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/all_returns_us_market_cap_quantile_ranges_cap_after_2003_longs_shorts_bar.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c391156",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/all_returns_us_market_cap_quantile_ranges_cap_all_dates_longs_shorts_bar.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf93374",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/all_returns_us_market_cap_quantile_ranges_cap_after_2003_bar.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3479b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/all_returns_us_market_cap_quantile_ranges_cap_all_dates_bar.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f0147",
   "metadata": {},
   "source": [
    "# Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_digest = all_digests_nogroup[(all_digests_nogroup[\"train_file\"] == \"all_dates\") &\n",
    "                                     (all_digests_nogroup[\"model\"] == \"all_ensemble\")]\n",
    "\n",
    "current_digest = current_digest[current_digest[\"date\"] >= pd.Timestamp(\"2005-01-01\")]\n",
    "\n",
    "current_quantile_ranges = [(0,1), (0.12,1), (0.34,1), (0.56,1), (0.78,1), (0.89,1)]\n",
    "\n",
    "current_digest = current_digest[current_digest[\"min_max_quantile\"].isin(current_quantile_ranges)]\n",
    "\n",
    "top_n_quantiles = 4\n",
    "\n",
    "n_stock_annot = True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "hatch = \"\"\n",
    "\n",
    "bar_width = 0.2\n",
    "\n",
    "text_offset = 0.002\n",
    "\n",
    "for i, quantile_range in enumerate(current_quantile_ranges):\n",
    "\n",
    "    current_current_digest = current_digest[current_digest[\"min_max_quantile\"] == quantile_range]\n",
    "    \n",
    "    if i == 0:\n",
    "        bar_us = ax.bar(height = current_current_digest[current_current_digest[\"region\"] == \"us\"][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "           x=i-0.2, width=bar_width, label=\"US\", color=\"tab:red\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_eu = ax.bar(height = current_current_digest[current_current_digest[\"region\"] == \"eu\"][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "           x=i, width=bar_width, label=\"Europe\", color=\"tab:blue\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_jp = ax.bar(height = current_current_digest[current_current_digest[\"region\"] == \"jp\"][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "           x=i+0.2, width=bar_width, label=\"Japan\", color=\"tab:green\", edgecolor=\"black\", hatch=hatch)\n",
    "\n",
    "    else:\n",
    "        bar_us = ax.bar(height = current_current_digest[current_current_digest[\"region\"] == \"us\"][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "           x=i-0.2, width=bar_width, color=\"tab:red\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_eu = ax.bar(height = current_current_digest[current_current_digest[\"region\"] == \"eu\"][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "           x=i, width=bar_width, color=\"tab:blue\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_jp = ax.bar(height = current_current_digest[current_current_digest[\"region\"] == \"jp\"][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "           x=i+0.2, width=bar_width, color=\"tab:green\", edgecolor=\"black\", hatch=hatch)\n",
    "        \n",
    "    if n_stock_annot:\n",
    "        n_stocks_in_group = current_current_digest[current_current_digest[\"region\"] == \"us\"][\"n_stocks_in_group\"].mean()\n",
    "        rect_us = bar_us.patches[0]\n",
    "        ax.annotate(text = f\"n = {int(n_stocks_in_group)}\", \n",
    "                   xy = (rect_us.get_x() + rect_us.get_width() / 2, \n",
    "                    rect_us.get_height() + text_offset), ha='center', va='center',\n",
    "                   size=16, rotation=90\n",
    "                   )\n",
    "        n_stocks_in_group = current_current_digest[current_current_digest[\"region\"] == \"eu\"][\"n_stocks_in_group\"].mean()\n",
    "        rect_eu = bar_eu.patches[0]\n",
    "        ax.annotate(text = f\"n = {int(n_stocks_in_group)}\", \n",
    "                   xy = (rect_eu.get_x() + rect_eu.get_width() / 2, \n",
    "                    rect_eu.get_height() + text_offset), ha='center', va='center',\n",
    "                   size=16, rotation=90\n",
    "                   )\n",
    "        n_stocks_in_group = current_current_digest[current_current_digest[\"region\"] == \"jp\"][\"n_stocks_in_group\"].mean()\n",
    "        rect_jp = bar_jp.patches[0]\n",
    "        ax.annotate(text = f\"n = {int(n_stocks_in_group)}\", \n",
    "                   xy = (rect_jp.get_x() + rect_jp.get_width() / 2, \n",
    "                    rect_jp.get_height() + text_offset), ha='center', va='center',\n",
    "                   size=16, rotation=90\n",
    "                   )\n",
    "    \n",
    "    \n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "ax.set_xticks(range(len(current_quantile_ranges)))\n",
    "ax.set_xticklabels(current_quantile_ranges)\n",
    "ax.legend(fontsize=18)\n",
    "ax.set_xlabel(\"Market Cap Quantile Ranges\", fontsize = 20)\n",
    "\n",
    "ax.set_ylim(top=0.021)\n",
    "plt.tight_layout()\n",
    "ax.grid(axis=\"y\")\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/all_returns_regions_mc_cap_ranges_after_2005_top_10pct_bar.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aca774",
   "metadata": {},
   "source": [
    "# Returns in bear/bull and rec/exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ab0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nber_rec_dates = pd.read_csv(\"../time_periods/model_train_ready/nber_recession_dates.csv\")\n",
    "nber_rec_dates[\"date\"] = pd.to_datetime(nber_rec_dates[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nber_exp_dates = pd.read_csv(\"../time_periods/model_train_ready/nber_expansion_dates.csv\")\n",
    "nber_exp_dates[\"date\"] = pd.to_datetime(nber_exp_dates[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba076d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_bear_dates = pd.read_csv(\"../time_periods/model_train_ready/bear_dates_sp500.csv\")\n",
    "sp500_bear_dates[\"date\"] = pd.to_datetime(sp500_bear_dates[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ab6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_bull_dates = pd.read_csv(\"../time_periods/model_train_ready/bull_dates_sp500.csv\")\n",
    "sp500_bull_dates[\"date\"] = pd.to_datetime(sp500_bull_dates[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7285e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_dict = {\n",
    "    \"nber_recession_dates\" : \"NBER Recession\",\n",
    "    \"nber_expansion_dates\" : \"NBER Expansion\",\n",
    "    \"bear_dates_sp500\" : \"Qualitative Bear\",\n",
    "    \"bull_dates_sp500\" : \"Qualitative Bull\",\n",
    "    \"non_bear_dates_sp500\" : \"Qualitative Non-Bear\",\n",
    "    \"flat_dates_sp500\" : \"Qualitative Flat\",\n",
    "    \"markov_rec\" : \"Markov Recession\",\n",
    "    \"markov_exp\" : \"Markov Expansion\",\n",
    "    \"return_filter_bear_m_short_2_3\" : \"Negative Filter (ST)\",\n",
    "    \"return_filter_bear_m_long_3_6_\" : \"Negative Filter (LT)\",\n",
    "    \"return_filter_bull_m_short_2_3\" : \"Positive Filter (ST)\",\n",
    "    \"return_filter_bull_m_long_3_6_\" : \"Positive Filter (LT)\",\n",
    "    \"EPU_rec_2yr\" : \"EPU Recession\",\n",
    "    \"EPU_exp_2yr\" : \"EPU Expansion\",\n",
    "    \"all_dates\" : \"All Dates\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ee967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039008db",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_quantiles = 4\n",
    "\n",
    "current_digest = all_digests_nogroup[(all_digests_nogroup[\"model\"] == \"all_ensemble\")]\n",
    "\n",
    "current_digest = current_digest[current_digest[\"region\"] == \"us\"]\n",
    "\n",
    "current_digest = current_digest[current_digest[\"min_max_quantile\"] == (0.78, 1)]\n",
    "\n",
    "current_digest = current_digest[current_digest[\"date\"] >= pd.Timestamp(\"2020-02-21\")]\n",
    "current_digest = current_digest[current_digest[\"date\"] <= pd.Timestamp(\"2020-03-20\")]\n",
    "\n",
    "\n",
    "n_stock_annot = False\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "grouped_df = current_digest.groupby([\"train_file\"])[[f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\", f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\", f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\", \"n_stocks_in_group\"]].mean()\n",
    "grouped_df.sort_values(f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\", ascending=False, inplace=True)\n",
    "\n",
    "grouped_df.index = grouped_df.index.map(train_file_dict)\n",
    "\n",
    "bar_width = 0.20\n",
    "\n",
    "for i, train_file in enumerate(grouped_df.index):\n",
    "    hatch = None\n",
    "    if \" Bear\" in train_file or \"Recession\" in train_file or \"Negative\" in train_file:\n",
    "        hatch = \"\\\\\\\\\"\n",
    "    if \"Flat\" in train_file or \"All\" in train_file:\n",
    "        hatch = '..'\n",
    "    if i == 0:\n",
    "        bar_all = ax.bar(height = grouped_df[grouped_df.index == train_file][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"], \n",
    "           x=i-0.2, width=bar_width, label=\"Total\", color=\"tab:purple\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_longs = ax.bar(height = grouped_df[grouped_df.index == train_file][f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\"], \n",
    "           x=i, width=bar_width, label=\"Longs\", color=\"tab:blue\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_shorts = ax.bar(height = grouped_df[grouped_df.index == train_file][f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"], \n",
    "           x=i+0.2, width=bar_width, label=\"Shorts\", color=\"tab:orange\", edgecolor=\"black\", hatch=hatch)\n",
    "\n",
    "    else:\n",
    "        bar_all = ax.bar(height = grouped_df[grouped_df.index == train_file][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"], \n",
    "           x=i-0.2, width=bar_width, color=\"tab:purple\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_longs = ax.bar(height = grouped_df[grouped_df.index == train_file][f\"top_{top_n_quantiles}_trr_5_fwd_ar_mean\"], \n",
    "           x=i, width=bar_width, color=\"tab:blue\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_shorts = ax.bar(height = grouped_df[grouped_df.index == train_file][f\"bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"], \n",
    "           x=i+0.2, width=bar_width, color=\"tab:orange\", edgecolor=\"black\", hatch=hatch)\n",
    "    \n",
    "\n",
    "if n_stock_annot:\n",
    "    n_stocks_in_group = grouped_df[\"n_stocks_in_group\"]\n",
    "\n",
    "    for i, bar in enumerate(ax.patches):\n",
    "        ax.annotate(text = f\"n = {int(n_stocks_in_group.iloc[i])}\", \n",
    "                   xy = (bar.get_x() + bar.get_width() / 2, \n",
    "                    bar.get_height() + 0.0015), ha='center', va='center',\n",
    "                   size=12\n",
    "                   )\n",
    "\n",
    "plt.axhline(0, color=\"black\", lw=1)\n",
    "\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "ax.tick_params(axis='both', which='major', labelsize=20, labelbottom=True)\n",
    "ax.set_xticks(range(len(grouped_df.index)))\n",
    "ax.set_xticklabels(grouped_df.index, rotation=45, ha='right')\n",
    "ax.legend(fontsize=16)\n",
    "leg = ax.get_legend()\n",
    "leg.legend_handles[0].set_hatch(\"\")\n",
    "leg.legend_handles[1].set_hatch(\"\")\n",
    "leg.legend_handles[2].set_hatch(\"\")\n",
    "\n",
    "avg_trr_5_ar = us_lookup_mc_cap[us_lookup_mc_cap[\"date\"].dt.dayofweek == 4]\n",
    "avg_trr_5_ar = avg_trr_5_ar[avg_trr_5_ar[\"date\"].isin(current_digest[\"date\"].unique())].groupby(\"date\")[\"trr_5_ar\"].mean().mean()\n",
    "ax.axhline(avg_trr_5_ar, color=\"black\", linestyle=\"--\", lw=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "ax.grid(axis=\"y\")\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28625ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/train_file_ensemble_returns_us_nber_rec_over_078_after_2003_bar.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1fcdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/train_file_ensemble_returns_us_nber_exp_over_078_after_2003_bar.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/train_file_ensemble_returns_us_qual_bear_over_078_after_2003_bar.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/train_file_ensemble_returns_us_qual_bull_over_078_after_2003_bar.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba431b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/train_file_ensemble_returns_us_all_dates_over_078_after_2003_bar.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda31ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/train_file_ensemble_returns_us_covid_over_078_bar.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4b69d8",
   "metadata": {},
   "source": [
    "# Adaptive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a01eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc8e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date_files = os.listdir(\"../time_periods/model_test_ready/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "bull_rec_test_files = [x for x in test_date_files if (\"bull\" in x or \"exp\" in x or \"non_bear\" in x or \"non_rec\" in x) \n",
    "                       and (\"non_exp\" not in x) and (\"non_bull\" not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae16d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bear_rec_test_files = [x for x in test_date_files if (\"bear\" in x or \"rec\" in x or \"non_bull\" in x or \"non_exp\" in x) \n",
    "                       and (\"non_rec\" not in x) and (\"non_bear\" not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed830c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_names = current_digest[\"train_file\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bull_rec_train_files = [x for x in train_file_names if (\"bull\" in x or \"exp\" in x or \"non_bear\" in x or \"non_rec\" in x) \n",
    "                       and (\"non_exp\" not in x) and (\"non_bull\" not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3886a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bear_rec_train_files = [x for x in train_file_names if (\"bear\" in x or \"rec\" in x or \"non_bull\" in x or \"non_exp\" in x) \n",
    "                       and (\"non_rec\" not in x) and (\"non_bear\" not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b7025",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a278c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name_dict = {\n",
    "    'nber_recession_dates_class_lstm_ba4da75c' : \"Recession Class LSTM\",\n",
    "    'nber_non_recession_dates_class_lstm_ba4da75c' : \"Non-Recession Class LSTM\",\n",
    "    'bear_dates_qbear_class_lstm_9046df4a' : \"Bear Class LSTM\",\n",
    "    'bull_dates_qbull_class_lstm_f241ab59' : \"Bull Class LSTM\",\n",
    "    'non_bear_dates_qbear_class_lstm_9046df4a' : \"Non-Bear Class LSTM\",\n",
    "    'non_bull_dates_bqull_class_lstm_f241ab59' : \"Non-Bull Class LSTM\",\n",
    "    'markov_rec_dates_test_all_years_order1_4_10_5yr_avg' : \"Markov Recession\",\n",
    "    'markov_exp_dates_test_all_years_order1_4_10_5yr_avg' : \"Markov Expansion\",\n",
    "    'return_filter_bull_m_long_3_6_' : \"Positive Filter (LT)\",\n",
    "    'return_filter_bull_m_short_2_3' : \"Positive Filter (ST)\",\n",
    "    'bear_lstm_mc_change_class' : \"Negative MC Change LSTM\",\n",
    "    'bull_lstm_mc_change_class' : \"Positive MC Change LSTM\",\n",
    "    'return_filter_bear_m_long_3_6_12' : \"Negative Filter (LT)\",\n",
    "    'return_filter_bear_m_short_2_3' : \"Negative Filter (ST)\",\n",
    "    'return_filter_bull_m_long_3_6_12' : \"Positive Filter (LT)\",\n",
    "    'return_filter_bull_m_short_2_3' : \"Positive Filter (ST)\",  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name_dict = {\n",
    "    'nber_recession_dates' : \"NBER Recession\",\n",
    "    'nber_expansion_dates' : \"NBER Expansion\",\n",
    "    'bear_dates_sp500' : \"Qualitative Bear\",\n",
    "    'bull_dates_sp500' : \"Qualitative Bull\",\n",
    "    'non_bear_dates_sp500' : \"Qualitative Non-Bear\",\n",
    "    'flat_dates_sp500' : \"Qualitative Flat\",\n",
    "    'return_filter_bear_m_long_3_6_' : \"Negative Filter (LT)\",\n",
    "    'return_filter_bear_m_long_3_6_12' : \"Negative Filter (LT)\",\n",
    "    'return_filter_bear_m_short_2_3' : \"Negative Filter (ST)\",\n",
    "    'return_filter_bull_m_long_3_6_' : \"Positive Filter (LT)\",\n",
    "    'return_filter_bull_m_long_3_6_12' : \"Positive Filter (LT)\",\n",
    "    'return_filter_bull_m_short_2_3' : \"Positive Filter (ST)\",\n",
    "    'markov_rec' : \"Markov Recession\",\n",
    "    'markov_exp' : \"Markov Expansion\",\n",
    "    'markov_rec_dates_train_2020_order1_4_10_smooth_5yr_avg' : \"Markov Recession\",\n",
    "    'markov_exp_dates_train_2020_order1_4_10_smooth_5yr_avg' : \"Markov Expansion\",\n",
    "    'EPU_rec_2yr' : \"EPU Recession\",\n",
    "    'EPU_exp_2yr' : \"EPU Expansion\",\n",
    "    'all_dates' : \"All Dates\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cfcfc",
   "metadata": {},
   "source": [
    "### Test on predicted dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_digest = all_digests_nogroup\n",
    "\n",
    "current_digest = all_digests_nogroup[all_digests_nogroup[\"model\"] == \"all_ensemble\"]\n",
    "current_digest = current_digest[current_digest[\"region\"] == \"us\"]\n",
    "current_digest = current_digest[current_digest[\"min_max_quantile\"] == (0.78, 1)]\n",
    "current_digest = current_digest[current_digest[\"date\"] >= pd.Timestamp(\"2003-01-01\")]\n",
    "\n",
    "#current_digest = current_digest[~((current_digest[\"date\"] >= pd.Timestamp(\"2019-12-31\")) & (current_digest[\"date\"] <= pd.Timestamp(\"2020-05-31\")))]\n",
    "current_digest = current_digest[current_digest[\"date\"] <= pd.Timestamp(\"2019-12-31\")]\n",
    "\n",
    "feature = \"top_minus_bottom_4_trr_5_fwd_ar_mean\"\n",
    "\n",
    "flip_filters = True\n",
    "\n",
    "current_bear_rec_train_files = bear_rec_train_files\n",
    "current_bull_rec_train_files = bull_rec_train_files\n",
    "\n",
    "current_bear_rec_test_files = bear_rec_test_files\n",
    "current_bull_rec_test_files = bull_rec_test_files\n",
    "\n",
    "subtract_average = False\n",
    "\n",
    "times_dates = False\n",
    "\n",
    "\n",
    "\n",
    "if flip_filters:\n",
    "    bear_filters = [x for x in current_bear_rec_train_files if \"filter\" in x]\n",
    "    bull_filters = [x for x in current_bull_rec_train_files if \"filter\" in x]\n",
    "    current_bear_rec_train_files = [x for x in current_bear_rec_train_files if \"filter\" not in x] + bull_filters\n",
    "    current_bull_rec_train_files = [x for x in current_bull_rec_train_files if \"filter\" not in x] + bear_filters\n",
    "    \n",
    "\n",
    "    \n",
    "current_test_date_files = sorted(current_bear_rec_test_files)# + [\"return_filter_bull_m_long_3_6_12.csv\", \"return_filter_bull_m_short_2_3.csv\"]\n",
    "#current_test_date_files = sorted(current_bull_rec_test_files)# + [\"return_filter_bear_m_long_3_6_12.csv\", \"return_filter_bear_m_short_2_3.csv\"]\n",
    "\n",
    "\n",
    "\n",
    "current_train_file_names = sorted(current_bear_rec_train_files) + sorted(current_bull_rec_train_files) + [\"flat_dates_sp500\", \"all_dates\"]\n",
    "\n",
    "current_test_file_names = [x.split(\".csv\")[0] for x in current_test_date_files]\n",
    "\n",
    "results_df = pd.DataFrame(index=current_train_file_names, columns=current_test_file_names + [\"Mean of Row\", \"All Dates\"\n",
    "                                                                                             , \"Qualitative Bear\"\n",
    "                                                                                             #, \"Qualitative Bull\"\n",
    "                                                                                            ])\n",
    "\n",
    "for i, train_file_name in enumerate(current_train_file_names):\n",
    "    current_row = []\n",
    "    for test_file_path in current_test_date_files:\n",
    "        \n",
    "        current_current_digest = current_digest[current_digest[\"train_file\"] == train_file_name]\n",
    "        \n",
    "        current_test_dates = pd.read_csv(f\"../time_periods/model_test_ready/{test_file_path}\")\n",
    "        current_test_dates[\"date\"] = pd.to_datetime(current_test_dates[\"date\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        if subtract_average:\n",
    "            avg_return = (current_current_digest[current_current_digest[\"date\"].isin(current_test_dates[\"date\"])].set_index(\"date\")[feature] - us_lookup_mc_cap_avg[us_lookup_mc_cap_avg.index.isin(current_test_dates[\"date\"])]).mean()\n",
    "        else:\n",
    "            avg_return = current_current_digest[current_current_digest[\"date\"].isin(current_test_dates[\"date\"])][feature].mean()\n",
    "\n",
    "        if times_dates:\n",
    "            avg_return = avg_return*current_current_digest[current_current_digest[\"date\"].isin(current_test_dates[\"date\"])][\"date\"].nunique()\n",
    "        current_row.append(avg_return)\n",
    "        \n",
    "    current_row.append(float(sum(current_row)/len(current_row)))\n",
    "    \n",
    "    current_current_digest = current_digest[current_digest[\"train_file\"] == train_file_name]\n",
    "    avg_return_all_dates = current_current_digest[feature].mean()\n",
    "    if subtract_average:\n",
    "        avg_return_all_dates = (current_current_digest.set_index(\"date\")[feature] - us_lookup_mc_cap_avg[us_lookup_mc_cap_avg.index.isin(current_current_digest[\"date\"])]).mean()\n",
    "    else:\n",
    "        avg_return_all_dates = current_current_digest[feature].mean()\n",
    "\n",
    "    current_row.append(avg_return_all_dates)\n",
    "    \n",
    "    qual_bear_dates = pd.read_csv(\"../time_periods/model_train_ready/bear_dates_sp500.csv\")\n",
    "    qual_bear_dates[\"date\"] = pd.to_datetime(qual_bear_dates[\"date\"])\n",
    "    avg_qual_bear_returns = current_current_digest[current_current_digest[\"date\"].isin(qual_bear_dates[\"date\"])][feature].mean()\n",
    "    current_row.append(avg_qual_bear_returns)\n",
    "    \n",
    "    qual_bull_dates = pd.read_csv(\"../time_periods/model_train_ready/bull_dates_sp500.csv\")\n",
    "    qual_bull_dates[\"date\"] = pd.to_datetime(qual_bull_dates[\"date\"])\n",
    "    avg_qual_bull_returns = current_current_digest[current_current_digest[\"date\"].isin(qual_bull_dates[\"date\"])][feature].mean()\n",
    "    \n",
    "    results_df.loc[train_file_name] = current_row\n",
    "results_df = results_df.astype(float)\n",
    "\n",
    "results_df.rename(columns = test_file_name_dict, index=train_name_dict, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,9))\n",
    "\n",
    "ax = sb.heatmap(results_df, cmap=\"flare\", annot=True, cbar=False, square=False, annot_kws={\"fontsize\": 11}, fmt='.2%')\n",
    "\n",
    "\n",
    "ax.add_patch(Rectangle((0.02, 0.02), 8.96, 5.94, fill=False, edgecolor='tab:red', lw=2))\n",
    "ax.add_patch(Rectangle((0.02, 6.02), 8.96, 6.95, fill=False, edgecolor='tab:blue', lw=2))\n",
    "ax.add_patch(Rectangle((0.02, 13.02), 8.96, 1.93, fill=False, edgecolor='tab:green', lw=2))\n",
    "\n",
    "ax.set_ylabel('Train dates', fontsize = 15)\n",
    "ax.set_xlabel('Test dates', fontsize = 15)\n",
    "\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "ax.tick_params(axis='both', which='major', labelsize=12, labelbottom=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb1903",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/ensemble_us_returns_train_and_predict_bear_dates_after_2003_before_2020_filterflip_with_qual_bear.pdf\", dpi=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99520575",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/ensemble_us_returns_train_and_predict_bear_dates_after_2003_excl_2020firsthalf_filterflip_with_qual_bear.pdf\", dpi=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/ensemble_eu_returns_train_and_predict_bear_dates_after_2005_no_filterflip_with_qual_bear.pdf\", dpi=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81324b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/ensemble_jp_returns_train_and_predict_bear_dates_after_2005_no_filterflip_with_qual_bear_top_2p5pct.pdf\", dpi=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ad2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/ensemble_eu_returns_train_and_predict_bull_dates_after_2003_filterflip_with_qual_bull.pdf\", dpi=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee00ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/ensemble_jp_returns_train_and_predict_bull_dates_after_2003_filterflip_with_qual_bull.pdf\", dpi=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff4269",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date_files = os.listdir(\"../time_periods/model_train_ready/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date_files = train_date_files + [\"markov_rec_dates_train_2020_order1_4_10_smooth_5yr_avg.csv\", \"markov_exp_dates_train_2020_order1_4_10_smooth_5yr_avg.csv\"]\n",
    "                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b55b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "bull_rec_train_date_files = [x for x in train_date_files if (\"bull\" in x or \"exp\" in x or \"non_bear\" in x or \"non_rec\" in x) \n",
    "                       and (\"non_exp\" not in x) and (\"non_bull\" not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b993cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "bear_rec_train_date_files = [x for x in train_date_files if (\"bear\" in x or \"rec\" in x or \"non_bull\" in x or \"non_exp\" in x) \n",
    "                       and (\"non_rec\" not in x) and (\"non_bear\" not in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a1b52",
   "metadata": {},
   "source": [
    "### Test on training dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_digest = all_digests_nogroup\n",
    "\n",
    "current_digest = all_digests_nogroup[all_digests_nogroup[\"model\"] == \"all_ensemble\"]\n",
    "current_digest = current_digest[current_digest[\"region\"] == \"eu\"]\n",
    "current_digest = current_digest[current_digest[\"min_max_quantile\"] == (0.78, 1)]\n",
    "current_digest = current_digest[current_digest[\"date\"] >= pd.Timestamp(\"2003-01-01\")]\n",
    "#current_digest = current_digest[current_digest[\"date\"] <= pd.Timestamp(\"2020-01-01\")]\n",
    "\n",
    "\n",
    "feature = \"top_minus_bottom_4_trr_5_fwd_ar_mean\"\n",
    "\n",
    "flip_filters = False\n",
    "\n",
    "current_bear_rec_train_files = bear_rec_train_files\n",
    "current_bull_rec_train_files = bull_rec_train_files\n",
    "\n",
    "current_bear_rec_train_date_files = bear_rec_train_date_files\n",
    "current_bull_rec_train_date_files = bull_rec_train_date_files\n",
    "\n",
    "\n",
    "\n",
    "if flip_filters:\n",
    "    bear_filters = [x for x in current_bear_rec_train_files if \"filter\" in x]\n",
    "    bull_filters = [x for x in current_bull_rec_train_files if \"filter\" in x]\n",
    "    current_bear_rec_train_files = [x for x in current_bear_rec_train_files if \"filter\" not in x] + bull_filters\n",
    "    current_bull_rec_train_files = [x for x in current_bull_rec_train_files if \"filter\" not in x] + bear_filters\n",
    "    \n",
    "current_train_date_files = sorted(current_bear_rec_train_date_files)\n",
    "#current_train_date_files = sorted(current_bull_rec_train_date_files)\n",
    "\n",
    "current_train_date_files = sorted(current_bear_rec_train_date_files) + sorted(current_bull_rec_train_date_files)\n",
    "\n",
    "\n",
    "\n",
    "current_train_file_names = sorted(current_bear_rec_train_files) + sorted(current_bull_rec_train_files) + [\"flat_dates_sp500\", \"all_dates\"]\n",
    "\n",
    "current_train_date_files = current_train_date_files + [\"flat_dates_sp500.csv\", \"all_dates.csv\"]\n",
    "\n",
    "current_train_date_file_names = [x.split(\".csv\")[0] for x in current_train_date_files]\n",
    "\n",
    "results_df = pd.DataFrame(index=current_train_file_names, columns=current_train_date_file_names)\n",
    "\n",
    "for i, train_file_name in enumerate(current_train_file_names):\n",
    "    current_row = []\n",
    "    for train_date_file_path in current_train_date_files:\n",
    "        \n",
    "        current_current_digest = current_digest[current_digest[\"train_file\"] == train_file_name]\n",
    "        \n",
    "        if \"markov\" in train_date_file_path:\n",
    "            current_train_dates = pd.read_csv(f\"../time_periods/model_train_ready_before_test/{train_date_file_path}\")\n",
    "        else:\n",
    "            current_train_dates = pd.read_csv(f\"../time_periods/model_train_ready/{train_date_file_path}\")\n",
    "        current_train_dates[\"date\"] = pd.to_datetime(current_train_dates[\"date\"])\n",
    "                \n",
    "        avg_return = current_current_digest[current_current_digest[\"date\"].isin(current_train_dates[\"date\"])][feature].mean()\n",
    "\n",
    "        current_row.append(avg_return)\n",
    "    \n",
    "    results_df.loc[train_file_name] = current_row \n",
    "results_df = results_df.astype(float)\n",
    "\n",
    "results_df.rename(columns = train_name_dict, index=train_name_dict, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,9))\n",
    "\n",
    "ax = sb.heatmap(results_df, cmap=\"flare\", annot=True, cbar=False, square=False, annot_kws={\"fontsize\": 10}, fmt='.2%')\n",
    "\n",
    "\n",
    "ax.add_patch(Rectangle((0.02, 0.02), 0.02, 5.92, fill=False, edgecolor='tab:red', lw=2))\n",
    "ax.add_patch(Rectangle((0.02, 6.02), 0.02, 6.93, fill=False, edgecolor='tab:blue', lw=2))\n",
    "ax.add_patch(Rectangle((0.02, 13.02), 0.02, 1.93, fill=False, edgecolor='tab:green', lw=2))\n",
    "\n",
    "ax.add_patch(Rectangle((0.02, 14.96), 5.92, 0.02, fill=False, edgecolor='tab:red', lw=2))\n",
    "ax.add_patch(Rectangle((6, 14.96), 7, 0.02, fill=False, edgecolor='tab:blue', lw=2))\n",
    "ax.add_patch(Rectangle((13.02, 14.96), 1.93, 0.02, fill=False, edgecolor='tab:green', lw=2))\n",
    "\n",
    "\n",
    "ax.set_ylabel('Train dates', fontsize = 15)\n",
    "ax.set_xlabel('Test dates', fontsize = 15)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "ax.tick_params(axis='both', which='major', labelsize=12, labelbottom=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef51213",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/ensemble_us_returns_train_and_test_dates_after_2003_no_filterflip.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afb74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/ensemble_eu_returns_train_and_test_dates_after_2003_no_filterflip_top_2p5pct.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5491f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/ensemble_jp_returns_train_and_test_dates_after_2003_no_filterflip_top_2p5pct.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25b5cd",
   "metadata": {},
   "source": [
    "### Mean of quadrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b7259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive, negative, flat heatmap, TRAIN DATES\n",
    "\n",
    "\n",
    "current_digest = all_digests_nogroup\n",
    "\n",
    "current_digest = all_digests_nogroup[all_digests_nogroup[\"model\"] == \"all_ensemble\"]\n",
    "current_digest = current_digest[current_digest[\"region\"] == \"us\"]\n",
    "current_digest = current_digest[current_digest[\"min_max_quantile\"] == (0.78, 1)]\n",
    "current_digest = current_digest[current_digest[\"date\"] >= pd.Timestamp(\"2005-01-01\")]\n",
    "current_digest = current_digest[current_digest[\"date\"] <= pd.Timestamp(\"2020-01-01\")]\n",
    "\n",
    "\n",
    "feature = \"top_minus_bottom_4_trr_5_fwd_ar_mean\"\n",
    "\n",
    "#train_file_names = current_digest[\"train_file\"].unique()\n",
    "\n",
    "include_filters = False\n",
    "\n",
    "flip_filters = False\n",
    "\n",
    "current_bear_rec_train_files = bear_rec_train_files\n",
    "current_bull_rec_train_files = bull_rec_train_files\n",
    "\n",
    "current_bear_rec_train_date_files = bear_rec_train_date_files\n",
    "current_bull_rec_train_date_files = bull_rec_train_date_files\n",
    "\n",
    "\n",
    "\n",
    "if flip_filters:\n",
    "    bear_filters = [x for x in current_bear_rec_train_files if \"filter\" in x]\n",
    "    bull_filters = [x for x in current_bull_rec_train_files if \"filter\" in x]\n",
    "    current_bear_rec_train_files = [x for x in current_bear_rec_train_files if \"filter\" not in x] + bull_filters\n",
    "    current_bull_rec_train_files = [x for x in current_bull_rec_train_files if \"filter\" not in x] + bear_filters\n",
    "    \n",
    "if not include_filters:\n",
    "    current_bear_rec_train_date_files = [x for x in current_bear_rec_train_date_files if \"filter\" not in x]\n",
    "    current_bull_rec_train_date_files = [x for x in current_bull_rec_train_date_files if \"filter\" not in x]\n",
    "\n",
    "current_train_file_categories = [sorted(current_bear_rec_train_date_files), sorted(current_bull_rec_train_date_files), \n",
    "                                # [\"flat_dates_sp500.csv\"], \n",
    "                                 [\"all_dates.csv\"]\n",
    "                                ]\n",
    "\n",
    "\n",
    "categories = [\"Neg.\", \"Pos.\", \n",
    "             # \"Flat\", \n",
    "              \"All\"\n",
    "             ]\n",
    "\n",
    "results_df = pd.DataFrame(index=categories, columns=categories)\n",
    "\n",
    "for i, current_train_file_category_train in enumerate(current_train_file_categories):\n",
    "    current_row = []\n",
    "    current_train_file_category_train_names = [x.split(\".\")[0] for x in current_train_file_category_train]\n",
    "    current_current_digest = current_digest[current_digest[\"train_file\"].isin(current_train_file_category_train_names)]\n",
    "    for train_date_file_paths_test in current_train_file_categories:\n",
    "        mean_value = 0\n",
    "        for train_date_file_path in train_date_file_paths_test:\n",
    "            \n",
    "            \n",
    "            \n",
    "            if \"markov\" in train_date_file_path:\n",
    "                current_train_dates = pd.read_csv(f\"../time_periods/model_train_ready_before_test/{train_date_file_path}\")\n",
    "            else:\n",
    "                current_train_dates = pd.read_csv(f\"../time_periods/model_train_ready/{train_date_file_path}\")\n",
    "            current_train_dates[\"date\"] = pd.to_datetime(current_train_dates[\"date\"])\n",
    "\n",
    "            avg_return = current_current_digest[current_current_digest[\"date\"].isin(current_train_dates[\"date\"])][feature].mean()\n",
    "            mean_value += avg_return\n",
    "        mean_value = mean_value / len(train_date_file_paths_test)\n",
    "        current_row.append(mean_value)\n",
    "\n",
    "    results_df.loc[categories[i]] = current_row \n",
    "results_df = results_df.astype(float)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "\n",
    "ax = sb.heatmap(results_df, cmap=\"flare\", annot=True, cbar=False, square=False, annot_kws={\"fontsize\": 22}, fmt='.2%')\n",
    "\n",
    "\n",
    "\n",
    "ax.set_ylabel('Train dates', fontsize = 21)\n",
    "ax.set_xlabel('Test dates', fontsize = 21)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "ax.tick_params(axis='both', which='major', labelsize=20, labelbottom=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4930f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/mean_returns_train_dates_and_test_dates_small_no_filters_jp_after_2005_top_2p5pct.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a7710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/mean_returns_train_dates_and_test_dates_small_no_filters_eu_after_2005_top_2p5pct.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42810cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/mean_returns_train_dates_and_test_dates_small_no_filters_us_after_2005_top_2p5pct.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1384dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive, negative, flat heatmap TEST DATES\n",
    "\n",
    "\n",
    "current_digest = all_digests_nogroup\n",
    "\n",
    "current_digest = all_digests_nogroup[all_digests_nogroup[\"model\"] == \"all_ensemble\"]\n",
    "current_digest = current_digest[current_digest[\"region\"] == \"jp\"]\n",
    "current_digest = current_digest[current_digest[\"min_max_quantile\"] == (0.78, 1)]\n",
    "current_digest = current_digest[current_digest[\"date\"] >= pd.Timestamp(\"2005-01-01\")]\n",
    "#current_digest = current_digest[current_digest[\"date\"] <= pd.Timestamp(\"2020-01-01\")]\n",
    "\n",
    "\n",
    "feature = \"top_minus_bottom_4_trr_5_fwd_ar_mean\"\n",
    "\n",
    "include_filters = False\n",
    "\n",
    "flip_filters = False\n",
    "\n",
    "current_bear_rec_train_files = bear_rec_train_files\n",
    "current_bull_rec_train_files = bull_rec_train_files\n",
    "\n",
    "current_bear_rec_train_date_files = bear_rec_train_date_files\n",
    "current_bull_rec_train_date_files = bull_rec_train_date_files\n",
    "\n",
    "\n",
    "\n",
    "if flip_filters:\n",
    "    bear_filters = [x for x in current_bear_rec_train_files if \"filter\" in x]\n",
    "    bull_filters = [x for x in current_bull_rec_train_files if \"filter\" in x]\n",
    "    current_bear_rec_train_files = [x for x in current_bear_rec_train_files if \"filter\" not in x] + bull_filters\n",
    "    current_bull_rec_train_files = [x for x in current_bull_rec_train_files if \"filter\" not in x] + bear_filters\n",
    "    \n",
    "if not include_filters:\n",
    "    current_bear_rec_train_date_files = [x for x in current_bear_rec_train_date_files if \"filter\" not in x]\n",
    "    current_bull_rec_train_date_files = [x for x in current_bull_rec_train_date_files if \"filter\" not in x]\n",
    "\n",
    "current_train_file_categories = [sorted(current_bear_rec_train_date_files), sorted(current_bull_rec_train_date_files), \n",
    "                                # [\"flat_dates_sp500.csv\"], \n",
    "                                 [\"all_dates.csv\"]\n",
    "                                ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "categories = [\"Neg.\", \"Pos.\", \n",
    "             # \"Flat\", \n",
    "              \"All\"\n",
    "             ]\n",
    "\n",
    "results_df = pd.DataFrame(index=categories, columns=categories)\n",
    "\n",
    "for i, current_train_file_category_train in enumerate(current_train_file_categories):\n",
    "    current_row = []\n",
    "    current_train_file_category_train_names = [x.split(\".\")[0] for x in current_train_file_category_train]\n",
    "    current_current_digest = current_digest[current_digest[\"train_file\"].isin(current_train_file_category_train_names)]\n",
    "    for train_date_file_paths_test in current_train_file_categories:\n",
    "        mean_value = 0\n",
    "        for train_date_file_path in train_date_file_paths_test:\n",
    "            \n",
    "            \n",
    "            \n",
    "            if \"markov\" in train_date_file_path:\n",
    "                current_train_dates = pd.read_csv(f\"../time_periods/model_train_ready_before_test/{train_date_file_path}\")\n",
    "            else:\n",
    "                current_train_dates = pd.read_csv(f\"../time_periods/model_train_ready/{train_date_file_path}\")\n",
    "            current_train_dates[\"date\"] = pd.to_datetime(current_train_dates[\"date\"])\n",
    "\n",
    "            avg_return = current_current_digest[current_current_digest[\"date\"].isin(current_train_dates[\"date\"])][feature].mean()\n",
    "            mean_value += avg_return\n",
    "        mean_value = mean_value / len(train_date_file_paths_test)\n",
    "        current_row.append(mean_value)\n",
    "\n",
    "    results_df.loc[categories[i]] = current_row \n",
    "results_df = results_df.astype(float)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "\n",
    "ax = sb.heatmap(results_df, cmap=\"flare\", annot=True, cbar=False, square=False, annot_kws={\"fontsize\": 22}, fmt='.2%')\n",
    "\n",
    "ax.set_ylabel('Train dates', fontsize = 21)\n",
    "ax.set_xlabel('Test dates', fontsize = 21)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "ax.tick_params(axis='both', which='major', labelsize=20, labelbottom=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe1116f",
   "metadata": {},
   "source": [
    "# Reversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91738115",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_digest = all_digests_nogroup[(all_digests_nogroup[\"model\"] == \"all_ensemble\")]\n",
    "\n",
    "current_digest = current_digest[current_digest[\"date\"] >= pd.Timestamp(\"2005-01-01\")]\n",
    "\n",
    "train_files =  ['return_filter_bear_m_long_3_6_',\n",
    "    'return_filter_bear_m_short_2_3',\n",
    "    'return_filter_bull_m_long_3_6_',\n",
    "    'return_filter_bull_m_short_2_3']\n",
    "\n",
    "train_file_names = [train_name_dict[x] for x in train_files]\n",
    "\n",
    "top_n_quantiles = 4\n",
    "\n",
    "n_stock_annot = True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "hatch = \"\"\n",
    "\n",
    "bar_width = 0.2\n",
    "\n",
    "text_offset = 0.002\n",
    "\n",
    "for i, train_file in enumerate(train_files):\n",
    "\n",
    "    current_current_digest = current_digest[current_digest[\"train_file\"] == train_file]\n",
    "    \n",
    "    if i == 0:\n",
    "        bar_us = ax.bar(height = current_current_digest[current_current_digest[\"region\"] == \"us\"][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "           x=i-0.2, width=bar_width, label=\"US\", color=\"tab:red\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_eu = ax.bar(height = current_current_digest[current_current_digest[\"region\"] == \"eu\"][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "           x=i, width=bar_width, label=\"Europe\", color=\"tab:blue\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_jp = ax.bar(height = current_current_digest[current_current_digest[\"region\"] == \"jp\"][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "           x=i+0.2, width=bar_width, label=\"Japan\", color=\"tab:green\", edgecolor=\"black\", hatch=hatch)\n",
    "\n",
    "    else:\n",
    "        bar_us = ax.bar(height = current_current_digest[current_current_digest[\"region\"] == \"us\"][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "           x=i-0.2, width=bar_width, color=\"tab:red\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_eu = ax.bar(height = current_current_digest[current_current_digest[\"region\"] == \"eu\"][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "           x=i, width=bar_width, color=\"tab:blue\", edgecolor=\"black\", hatch=hatch)\n",
    "        bar_jp = ax.bar(height = current_current_digest[current_current_digest[\"region\"] == \"jp\"][f\"top_minus_bottom_{top_n_quantiles}_trr_5_fwd_ar_mean\"].mean(), \n",
    "           x=i+0.2, width=bar_width, color=\"tab:green\", edgecolor=\"black\", hatch=hatch)\n",
    "    \n",
    "    \n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "ax.set_xticks(range(len(train_files)))\n",
    "ax.set_xticklabels(train_file_names)\n",
    "ax.legend(fontsize=18)\n",
    "ax.set_xlabel(\"Market Cap Quantile Ranges\", fontsize = 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "ax.grid(axis=\"y\")\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../figures/filter_reversal_by_regions.pdf\", dpi=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba043c62",
   "metadata": {},
   "source": [
    "# t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ensemble_preds_us = pd.read_parquet(\"digests/['catboost', 'xgb', 'logreg', 'rf']_ensemble_USmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_all_preds_078_after_2003.parquet\", engine=\"pyarrow\")\n",
    "all_ensemble_preds_us[\"date\"] = pd.to_datetime(all_ensemble_preds_us[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b42ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ensemble_preds_eu = pd.read_parquet(\"digests/['catboost', 'xgb', 'logreg', 'rf']_ensemble_Europemax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_all_preds_078_after_2003.parquet\", engine=\"pyarrow\")\n",
    "all_ensemble_preds_eu[\"date\"] = pd.to_datetime(all_ensemble_preds_eu[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30afdd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ensemble_preds_jp = pd.read_parquet(\"digests/['catboost', 'xgb', 'logreg', 'rf']_ensemble_Japanmax_trr_5_fwd_ar_0.3min_trr_5_fwd_ar_-0.3_all_preds_078_after_2003.parquet\", engine=\"pyarrow\")\n",
    "all_ensemble_preds_jp[\"date\"] = pd.to_datetime(all_ensemble_preds_jp[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, ttest_rel, normaltest, skewtest, kurtosistest, skew, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name_dict = {\n",
    "    'nber_recession_dates_class_lstm_ba4da75c' : \"Recession Class LSTM\",\n",
    "    'nber_non_recession_dates_class_lstm_ba4da75c' : \"Non-Recession Class LSTM\",\n",
    "    'bear_dates_qbear_class_lstm_9046df4a' : \"Bear Class LSTM\",\n",
    "    'bull_dates_qbull_class_lstm_f241ab59' : \"Bull Class LSTM\",\n",
    "    'non_bear_dates_qbear_class_lstm_9046df4a' : \"Non-Bear Class LSTM\",\n",
    "    'non_bull_dates_bqull_class_lstm_f241ab59' : \"Non-Bull Class LSTM\",\n",
    "    'markov_rec_dates_test_all_years_order1_4_10_5yr_avg' : \"Markov Recession\",\n",
    "    'markov_exp_dates_test_all_years_order1_4_10_5yr_avg' : \"Markov Expansion\",\n",
    "    'return_filter_bull_m_long_3_6_' : \"Positive Filter (LT)\",\n",
    "    'return_filter_bull_m_short_2_3' : \"Positive Filter (ST)\",\n",
    "    'bear_lstm_mc_change_class' : \"Negative MC Change LSTM\",\n",
    "    'bull_lstm_mc_change_class' : \"Positive MC Change LSTM\",\n",
    "    'return_filter_bear_m_long_3_6_12' : \"Negative Filter (LT)\",\n",
    "    'return_filter_bear_m_short_2_3' : \"Negative Filter (ST)\",\n",
    "    'return_filter_bull_m_long_3_6_12' : \"Positive Filter (LT)\",\n",
    "    'return_filter_bull_m_short_2_3' : \"Positive Filter (ST)\",  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using all predictions:\n",
    "\n",
    "current_preds = all_ensemble_preds_eu.copy()\n",
    "\n",
    "min_date = pd.Timestamp(\"2003-01-01\")\n",
    "max_date = pd.Timestamp(\"2019-12-31\")\n",
    "\n",
    "current_region = \"eu\"\n",
    "\n",
    "\n",
    "current_preds = current_preds[current_preds[\"date\"] >= min_date]\n",
    "current_preds = current_preds[current_preds[\"date\"] <= max_date]\n",
    "\n",
    "#current_preds = current_preds[~((current_preds[\"date\"] >= pd.Timestamp(\"2019-12-31\")) & (current_preds[\"date\"] <= pd.Timestamp(\"2020-05-31\")))]\n",
    "\n",
    "all_dates = False\n",
    "adaptive_model = True\n",
    "\n",
    "long_short_every_week = False\n",
    "\n",
    "compare_with_all_dates = True\n",
    "\n",
    "\n",
    "current_train_file = \"all_dates\"\n",
    "test_date_file_name = \"\"\n",
    "\n",
    "buy_conviction_quantiles = [39,38,37,36]\n",
    "sell_conviction_quantiles = [0,1,2,3]\n",
    "\n",
    "current_train_files = [\n",
    "    #\"non_bear_dates_sp500\", \n",
    "    #\"non_bear_dates_sp500\", \n",
    "    \"all_dates\",\n",
    "    \"all_dates\",\n",
    "    \"all_dates\",\n",
    "    #\"non_bear_dates_sp500\",\n",
    "    #\"non_bear_dates_sp500\",\n",
    "    #\"non_bear_dates_sp500\",\n",
    "    #\"nber_expansion_dates\",\n",
    "    #\"bull_dates_sp500\",\n",
    "    #\"bull_dates_sp500\",\n",
    "    #\"bull_dates_sp500\"\n",
    "                      ]\n",
    "\n",
    "final_train_file = \"all_dates\"\n",
    "\n",
    "test_date_file_names = [\n",
    "    \"nber_recession_dates_class_lstm_ba4da75c\", \n",
    "    #\"bull_lstm_mc_change_class\"\n",
    "    \"bear_dates_qbear_class_lstm_9046df4a\",\n",
    "    \"return_filter_bear_m_short_2_3\", \n",
    "    #\"return_filter_bear_m_long_3_6_12\",\n",
    "    #\"non_bull_dates_bqull_class_lstm_f241ab59\"\n",
    "]\n",
    "\n",
    "comparison_train_file = \"all_dates\"\n",
    "\n",
    "\n",
    "\n",
    "if all_dates:\n",
    "    current_preds_adaptive = current_preds[current_preds[\"train_file\"] == current_train_files[0]]\n",
    "    current_preds_adaptive_buys = current_preds_adaptive[current_preds_adaptive[\"conviction_quantile\"].isin(buy_conviction_quantiles)]\n",
    "    current_preds_adaptive_sells = current_preds_adaptive[current_preds_adaptive[\"conviction_quantile\"].isin(sell_conviction_quantiles)]\n",
    "    \n",
    "    current_preds_adaptive_buys.sort_values(\"date\", inplace=True)\n",
    "    current_preds_adaptive_sells.sort_values(\"date\", inplace=True)\n",
    "\n",
    "\n",
    "elif adaptive_model:\n",
    "    for i in range(len(current_train_files)):\n",
    "        \n",
    "        test_dates = pd.read_csv(f\"../time_periods/model_test_ready/{test_date_file_names[i]}.csv\")\n",
    "        test_dates[\"date\"] = pd.to_datetime(test_dates[\"date\"])\n",
    "        if i != 0:\n",
    "            test_dates = test_dates[~test_dates[\"date\"].isin(current_preds_adaptive_buys[\"date\"])]\n",
    "        current_current_preds = current_preds[current_preds[\"date\"].isin(test_dates[\"date\"])]\n",
    "        print(f\"N dates adaptive state {i}:\", current_current_preds[\"date\"].nunique())\n",
    "        current_current_preds = current_current_preds[current_current_preds[\"train_file\"] == current_train_files[i]]\n",
    "        current_current_preds[\"train_file\"] = current_train_files[i]\n",
    "        current_current_preds[\"test_file\"] = test_date_file_names[i]\n",
    "        \n",
    "        current_current_pred_buys = current_current_preds[current_current_preds[\"conviction_quantile\"].isin(buy_conviction_quantiles)]\n",
    "        current_current_pred_sells = current_current_preds[current_current_preds[\"conviction_quantile\"].isin(sell_conviction_quantiles)]\n",
    "\n",
    "        if i == 0:\n",
    "            current_preds_adaptive_buys = current_current_pred_buys\n",
    "            current_preds_adaptive_sells = current_current_pred_sells\n",
    "            \n",
    "        else:\n",
    "            current_preds_adaptive_buys = pd.concat([current_preds_adaptive_buys, current_current_pred_buys])\n",
    "            current_preds_adaptive_sells = pd.concat([current_preds_adaptive_sells, current_current_pred_sells])\n",
    "            \n",
    "        print(f\"Length adaptive state buys {i}:\", current_current_pred_buys[\"trr_5_fwd_ar\"].shape)\n",
    "        print(f\"Returns adaptive state buys {i}:\", current_current_pred_buys[\"trr_5_fwd_ar\"].mean())\n",
    "        print(f\"Length adaptive state sells {i}:\", current_current_pred_sells[\"trr_5_fwd_ar\"].shape)\n",
    "        print(f\"Returns adaptive state sells {i}:\", current_current_pred_sells[\"trr_5_fwd_ar\"].mean())\n",
    "        \n",
    "    if long_short_every_week:\n",
    "        current_current_preds = current_preds[current_preds[\"train_file\"] == final_train_file]\n",
    "        current_current_preds = current_current_preds[~current_current_preds[\"date\"].isin(current_preds_adaptive_buys[\"date\"])]\n",
    "        print(f\"N dates adaptive state {i+1}:\", current_current_preds[\"date\"].nunique())\n",
    "        current_current_preds[\"train_file\"] = current_train_files[i]\n",
    "        current_current_preds[\"test_file\"] = test_date_file_names[i]\n",
    "        \n",
    "        current_current_pred_buys = current_current_preds[current_current_preds[\"conviction_quantile\"].isin(buy_conviction_quantiles)]\n",
    "        current_current_pred_sells = current_current_preds[current_current_preds[\"conviction_quantile\"].isin(sell_conviction_quantiles)]\n",
    "\n",
    "\n",
    "        current_preds_adaptive_buys = pd.concat([current_preds_adaptive_buys, current_current_pred_buys])\n",
    "        current_preds_adaptive_sells = pd.concat([current_preds_adaptive_sells, current_current_pred_sells])\n",
    "        \n",
    "        print(f\"Length adaptive state buys {i+1}:\", current_current_pred_buys[\"trr_5_fwd_ar\"].shape)\n",
    "        print(f\"Returns adaptive state buys {i+1}:\", current_current_pred_buys[\"trr_5_fwd_ar\"].mean())\n",
    "        print(f\"Length adaptive state sells {i+1}:\", current_current_pred_sells[\"trr_5_fwd_ar\"].shape)\n",
    "        print(f\"Returns adaptive state sells {i+1}:\", current_current_pred_sells[\"trr_5_fwd_ar\"].mean())\n",
    "        \n",
    "        \n",
    "    current_preds_adaptive_buys.sort_values(\"date\", inplace=True)\n",
    "    current_preds_adaptive_sells.sort_values(\"date\", inplace=True)\n",
    "    \n",
    "\n",
    "for i, group in enumerate(current_preds_adaptive_buys.groupby(\"date\")):\n",
    "    current_buys = current_preds_adaptive_buys[current_preds_adaptive_buys[\"date\"] == group[0]]\n",
    "    current_sells = current_preds_adaptive_sells[current_preds_adaptive_sells[\"date\"] == group[0]]\n",
    "    current_n_buys = current_buys[\"gvkey\"].nunique()\n",
    "    current_n_sells = current_sells[\"gvkey\"].nunique()\n",
    "        \n",
    "    current_n_pairs = min(current_n_buys, current_n_sells)\n",
    "    \n",
    "    \n",
    "    current_current_preds_returns = current_buys.set_index(\"date\")[\"trr_5_fwd_ar\"].head(current_n_pairs) - current_sells.set_index(\"date\")[\"trr_5_fwd_ar\"].head(current_n_pairs)\n",
    "    \n",
    "    if i == 0:\n",
    "        current_pred_returns = current_current_preds_returns\n",
    "    else:\n",
    "        current_pred_returns = pd.concat([current_pred_returns, current_current_preds_returns])\n",
    "\n",
    "if current_region == \"us\":\n",
    "    current_index = us_lookup_mc_cap_avg\n",
    "elif current_region == \"eu\":\n",
    "    current_index = eu_lookup_mc_cap_avg\n",
    "elif current_region == \"jp\":\n",
    "    current_index = jp_lookup_mc_cap_avg\n",
    "\n",
    "comparison_preds = current_preds[current_preds[\"train_file\"] == comparison_train_file].copy()\n",
    "\n",
    "comparison_buys = comparison_preds[comparison_preds[\"conviction_quantile\"].isin(buy_conviction_quantiles)]\n",
    "comparison_sells = comparison_preds[comparison_preds[\"conviction_quantile\"].isin(sell_conviction_quantiles)]\n",
    "\n",
    "for i, group in enumerate(comparison_buys.groupby(\"date\")):\n",
    "    current_buys = comparison_buys[comparison_buys[\"date\"] == group[0]]\n",
    "    current_sells = comparison_sells[comparison_sells[\"date\"] == group[0]]\n",
    "    current_n_buys = current_buys[\"gvkey\"].nunique()\n",
    "    current_n_sells = current_sells[\"gvkey\"].nunique()\n",
    "        \n",
    "    current_n_pairs = min(current_n_buys, current_n_sells)\n",
    "    \n",
    "    \n",
    "    current_comparison_returns = current_buys.set_index(\"date\")[\"trr_5_fwd_ar\"].head(current_n_pairs) - current_sells.set_index(\"date\")[\"trr_5_fwd_ar\"].head(current_n_pairs)\n",
    "    \n",
    "    if i == 0:\n",
    "        comparison_returns = current_comparison_returns\n",
    "    else:\n",
    "        comparison_returns = pd.concat([comparison_returns, current_comparison_returns])\n",
    "\n",
    "#comparison_returns = comparison_buys.set_index(\"date\")[\"trr_5_fwd_ar\"] - comparison_sells.set_index(\"date\")[\"trr_5_fwd_ar\"]\n",
    "\n",
    "\n",
    "if compare_with_all_dates:\n",
    "    comparison_returns = comparison_returns[comparison_returns.index >= min_date]\n",
    "    comparison_returns = comparison_returns[comparison_returns.index <= max_date]\n",
    "else:  \n",
    "    comparison_returns = comparison_returns[comparison_returns.index.isin(current_pred_returns.index)]\n",
    "\n",
    "print(\"Comparison n dates:\", comparison_returns.index.nunique())\n",
    "print(\"Pred n dates:\", current_pred_returns.index.nunique())\n",
    "\n",
    "print(\"Comparison investments:\", comparison_returns.shape)\n",
    "print(\"Pred investments:\", current_pred_returns.shape)\n",
    "print(\"pct overlap\", 100*current_pred_returns.shape[0]/comparison_returns.shape[0])\n",
    "\n",
    "print(\"Comparison return:\", comparison_returns.mean()*100)\n",
    "print(\"Pred return:\", current_pred_returns.mean()*100)\n",
    "print(\"Rel pred return:\", current_pred_returns.mean()*100 - comparison_returns.mean()*100)\n",
    "\n",
    "print(\"Comparison std:\", comparison_returns.std()*np.sqrt(52))\n",
    "print(\"Pred std:\", current_pred_returns.std()*np.sqrt(52))\n",
    "\n",
    "print(\"Comparison sharpe:\", (comparison_returns.mean()*100)/(comparison_returns.std()*np.sqrt(52)))\n",
    "print(\"Pred sharpe:\", (current_pred_returns.mean()*100)/(current_pred_returns.std()*np.sqrt(52)))\n",
    "\n",
    "print(\"Ind:\", ttest_ind(current_pred_returns.values, comparison_returns.values, equal_var=False, alternative=\"greater\"))\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Skew:\", skew(current_pred_returns.values))\n",
    "print(\"Skew test:\", skewtest(current_pred_returns.values, alternative=\"two-sided\"))\n",
    "\n",
    "print(\"Kurtosis:\", kurtosis(current_pred_returns.values))\n",
    "print(\"Kurtosis test:\", kurtosistest(current_pred_returns.values, alternative=\"two-sided\"))\n",
    "\n",
    "print(\"Comparison normal:\", normaltest(comparison_returns))\n",
    "print(\"Pred normal:\", normaltest(current_pred_returns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066df034",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name_dict = {\n",
    "    'nber_recession_dates' : \"NBER Recession\",\n",
    "    'nber_expansion_dates' : \"NBER Expansion\",\n",
    "    'bear_dates_sp500' : \"Qualitative Bear\",\n",
    "    'bull_dates_sp500' : \"Qualitative Bull\",\n",
    "    'non_bear_dates_sp500' : \"Qualitative Non-Bear\",\n",
    "    'return_filter_bear_m_long_3_6_12' : \"Negative Filter (LT)\",\n",
    "    'return_filter_bear_m_short_2_3' : \"Negative Filter (LT)\",\n",
    "    'return_filter_bull_m_long_3_6_12' : \"Positive Filter (LT)\",\n",
    "    'return_filter_bull_m_short_2_3' : \"Positive Filter (ST)\",\n",
    "    'EPU_rec_2yr' : \"EPU Recession\",\n",
    "    'EPU_exp_2yr' : \"EPU Expansion\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509037f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using all predictions, train dates:\n",
    "\n",
    "current_preds = all_ensemble_preds_us.copy()\n",
    "\n",
    "min_date = pd.Timestamp(\"2003-01-01\")\n",
    "max_date = pd.Timestamp(\"2023-12-31\")\n",
    "\n",
    "current_region = \"us\"\n",
    "\n",
    "\n",
    "current_preds = current_preds[current_preds[\"date\"] >= min_date]\n",
    "current_preds = current_preds[current_preds[\"date\"] <= max_date]\n",
    "\n",
    "all_dates = False\n",
    "adaptive_model = True\n",
    "\n",
    "long_short_every_week = True\n",
    "\n",
    "compare_with_all_dates = False\n",
    "\n",
    "\n",
    "current_train_file = \"all_dates\"\n",
    "test_date_file_name = \"\"\n",
    "\n",
    "buy_conviction_quantiles = [39,38,37,36]\n",
    "sell_conviction_quantiles = [0,1,2,3]\n",
    "\n",
    "current_train_files = [\n",
    "    #\"non_bear_dates_sp500\", \n",
    "    #\"non_bear_dates_sp500\", \n",
    "    #\"all_dates\",\n",
    "    \"bear_dates_sp500\",\n",
    "    #\"non_bear_dates_sp500\",\n",
    "    #\"non_bear_dates_sp500\",\n",
    "    #\"non_bear_dates_sp500\",\n",
    "    #\"nber_expansion_dates\",\n",
    "    \"bull_dates_sp500\",\n",
    "    #\"bull_dates_sp500\",\n",
    "    #\"bull_dates_sp500\"\n",
    "                      ]\n",
    "\n",
    "final_train_file = \"all_dates\"\n",
    "\n",
    "test_date_file_names = [\n",
    "    #'nber_recession_dates',\n",
    "    #'nber_expansion_dates',\n",
    "    'bear_dates_sp500',\n",
    "    'bull_dates_sp500',\n",
    "    #'non_bear_dates_sp500',\n",
    "    #'return_filter_bear_m_long_3_6_12',\n",
    "    #'return_filter_bear_m_short_2_3',\n",
    "    #'return_filter_bull_m_long_3_6_12',\n",
    "    #'return_filter_bull_m_short_2_3',\n",
    "    #'EPU_rec_2yr',\n",
    "    #'EPU_exp_2yr'\n",
    "]\n",
    "\n",
    "comparison_train_file = \"all_dates\"\n",
    "\n",
    "\n",
    "\n",
    "if all_dates:\n",
    "    current_preds_adaptive = current_preds[current_preds[\"train_file\"] == current_train_files[0]]\n",
    "    current_preds_adaptive_buys = current_preds_adaptive[current_preds_adaptive[\"conviction_quantile\"].isin(buy_conviction_quantiles)]\n",
    "    current_preds_adaptive_sells = current_preds_adaptive[current_preds_adaptive[\"conviction_quantile\"].isin(sell_conviction_quantiles)]\n",
    "    \n",
    "    current_preds_adaptive_buys.sort_values(\"date\", inplace=True)\n",
    "    current_preds_adaptive_sells.sort_values(\"date\", inplace=True)\n",
    "\n",
    "\n",
    "elif adaptive_model:\n",
    "    for i in range(len(current_train_files)):\n",
    "        \n",
    "        test_dates = pd.read_csv(f\"../time_periods/model_train_ready/{test_date_file_names[i]}.csv\")\n",
    "        test_dates[\"date\"] = pd.to_datetime(test_dates[\"date\"])\n",
    "        if i != 0:\n",
    "            test_dates = test_dates[~test_dates[\"date\"].isin(current_preds_adaptive_buys[\"date\"])]\n",
    "        current_current_preds = current_preds[current_preds[\"date\"].isin(test_dates[\"date\"])]\n",
    "        print(f\"N dates adaptive state {i}:\", current_current_preds[\"date\"].nunique())\n",
    "        current_current_preds = current_current_preds[current_current_preds[\"train_file\"] == current_train_files[i]]\n",
    "        current_current_preds[\"train_file\"] = current_train_files[i]\n",
    "        current_current_preds[\"test_file\"] = test_date_file_names[i]\n",
    "        \n",
    "        current_current_pred_buys = current_current_preds[current_current_preds[\"conviction_quantile\"].isin(buy_conviction_quantiles)]\n",
    "        current_current_pred_sells = current_current_preds[current_current_preds[\"conviction_quantile\"].isin(sell_conviction_quantiles)]\n",
    "\n",
    "        if i == 0:\n",
    "            current_preds_adaptive_buys = current_current_pred_buys\n",
    "            current_preds_adaptive_sells = current_current_pred_sells\n",
    "            \n",
    "        else:\n",
    "            current_preds_adaptive_buys = pd.concat([current_preds_adaptive_buys, current_current_pred_buys])\n",
    "            current_preds_adaptive_sells = pd.concat([current_preds_adaptive_sells, current_current_pred_sells])\n",
    "            \n",
    "        print(f\"Length adaptive state buys {i}:\", current_current_pred_buys[\"trr_5_fwd_ar\"].shape)\n",
    "        print(f\"Returns adaptive state buys {i}:\", current_current_pred_buys[\"trr_5_fwd_ar\"].mean())\n",
    "        print(f\"Length adaptive state sells {i}:\", current_current_pred_sells[\"trr_5_fwd_ar\"].shape)\n",
    "        print(f\"Returns adaptive state sells {i}:\", current_current_pred_sells[\"trr_5_fwd_ar\"].mean())\n",
    "        \n",
    "    if long_short_every_week:\n",
    "        current_current_preds = current_preds[current_preds[\"train_file\"] == final_train_file]\n",
    "        current_current_preds = current_current_preds[~current_current_preds[\"date\"].isin(current_preds_adaptive_buys[\"date\"])]\n",
    "        print(f\"N dates adaptive state {i+1}:\", current_current_preds[\"date\"].nunique())\n",
    "        current_current_preds[\"train_file\"] = current_train_files[i]\n",
    "        current_current_preds[\"test_file\"] = test_date_file_names[i]\n",
    "        \n",
    "        current_current_pred_buys = current_current_preds[current_current_preds[\"conviction_quantile\"].isin(buy_conviction_quantiles)]\n",
    "        current_current_pred_sells = current_current_preds[current_current_preds[\"conviction_quantile\"].isin(sell_conviction_quantiles)]\n",
    "\n",
    "\n",
    "        current_preds_adaptive_buys = pd.concat([current_preds_adaptive_buys, current_current_pred_buys])\n",
    "        current_preds_adaptive_sells = pd.concat([current_preds_adaptive_sells, current_current_pred_sells])\n",
    "        \n",
    "        print(f\"Length adaptive state buys {i+1}:\", current_current_pred_buys[\"trr_5_fwd_ar\"].shape)\n",
    "        print(f\"Returns adaptive state buys {i+1}:\", current_current_pred_buys[\"trr_5_fwd_ar\"].mean())\n",
    "        print(f\"Length adaptive state sells {i+1}:\", current_current_pred_sells[\"trr_5_fwd_ar\"].shape)\n",
    "        print(f\"Returns adaptive state sells {i+1}:\", current_current_pred_sells[\"trr_5_fwd_ar\"].mean())\n",
    "        \n",
    "        \n",
    "    current_preds_adaptive_buys.sort_values(\"date\", inplace=True)\n",
    "    current_preds_adaptive_sells.sort_values(\"date\", inplace=True)\n",
    "    \n",
    "\n",
    "for i, group in enumerate(current_preds_adaptive_buys.groupby(\"date\")):\n",
    "    current_buys = current_preds_adaptive_buys[current_preds_adaptive_buys[\"date\"] == group[0]]\n",
    "    current_sells = current_preds_adaptive_sells[current_preds_adaptive_sells[\"date\"] == group[0]]\n",
    "    current_n_buys = current_buys[\"gvkey\"].nunique()\n",
    "    current_n_sells = current_sells[\"gvkey\"].nunique()\n",
    "        \n",
    "    current_n_pairs = min(current_n_buys, current_n_sells)\n",
    "    \n",
    "    \n",
    "    current_current_preds_returns = current_buys.set_index(\"date\")[\"trr_5_fwd_ar\"].head(current_n_pairs) - current_sells.set_index(\"date\")[\"trr_5_fwd_ar\"].head(current_n_pairs)\n",
    "    \n",
    "    if i == 0:\n",
    "        current_pred_returns = current_current_preds_returns\n",
    "    else:\n",
    "        current_pred_returns = pd.concat([current_pred_returns, current_current_preds_returns])\n",
    "\n",
    "if current_region == \"us\":\n",
    "    current_index = us_lookup_mc_cap_avg\n",
    "elif current_region == \"eu\":\n",
    "    current_index = eu_lookup_mc_cap_avg\n",
    "elif current_region == \"jp\":\n",
    "    current_index = jp_lookup_mc_cap_avg\n",
    "\n",
    "\n",
    "comparison_preds = current_preds[current_preds[\"train_file\"] == comparison_train_file].copy()\n",
    "\n",
    "comparison_buys = comparison_preds[comparison_preds[\"conviction_quantile\"].isin(buy_conviction_quantiles)]\n",
    "comparison_sells = comparison_preds[comparison_preds[\"conviction_quantile\"].isin(sell_conviction_quantiles)]\n",
    "\n",
    "for i, group in enumerate(comparison_buys.groupby(\"date\")):\n",
    "    current_buys = comparison_buys[comparison_buys[\"date\"] == group[0]]\n",
    "    current_sells = comparison_sells[comparison_sells[\"date\"] == group[0]]\n",
    "    current_n_buys = current_buys[\"gvkey\"].nunique()\n",
    "    current_n_sells = current_sells[\"gvkey\"].nunique()\n",
    "        \n",
    "    current_n_pairs = min(current_n_buys, current_n_sells)\n",
    "    \n",
    "    \n",
    "    current_comparison_returns = current_buys.set_index(\"date\")[\"trr_5_fwd_ar\"].head(current_n_pairs) - current_sells.set_index(\"date\")[\"trr_5_fwd_ar\"].head(current_n_pairs)\n",
    "    \n",
    "    if i == 0:\n",
    "        comparison_returns = current_comparison_returns\n",
    "    else:\n",
    "        comparison_returns = pd.concat([comparison_returns, current_comparison_returns])\n",
    "\n",
    "if compare_with_all_dates:\n",
    "    comparison_returns = comparison_returns[comparison_returns.index >= min_date]\n",
    "    comparison_returns = comparison_returns[comparison_returns.index <= max_date]\n",
    "else:  \n",
    "    comparison_returns = comparison_returns[comparison_returns.index.isin(current_pred_returns.index)]\n",
    "\n",
    "print(\"Comparison n dates:\", comparison_returns.index.nunique())\n",
    "print(\"Pred n dates:\", current_pred_returns.index.nunique())\n",
    "\n",
    "print(\"Comparison investments:\", comparison_returns.shape)\n",
    "print(\"Pred investments:\", current_pred_returns.shape)\n",
    "\n",
    "print(\"Comparison return:\", comparison_returns.mean()*100)\n",
    "print(\"Pred return:\", current_pred_returns.mean()*100)\n",
    "print(\"Rel pred return:\", current_pred_returns.mean()*100 - comparison_returns.mean()*100)\n",
    "\n",
    "print(\"Comparison std:\", comparison_returns.std()*np.sqrt(52))\n",
    "print(\"Pred std:\", current_pred_returns.std()*np.sqrt(52))\n",
    "\n",
    "print(\"Comparison sharpe:\", (comparison_returns.mean()*100)/(comparison_returns.std()*np.sqrt(52)))\n",
    "print(\"Pred sharpe:\", (current_pred_returns.mean()*100)/(current_pred_returns.std()*np.sqrt(52)))\n",
    "\n",
    "print(\"Ind:\", ttest_ind(current_pred_returns.values, comparison_returns.values, equal_var=False, alternative=\"greater\"))\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Skew:\", skew(current_pred_returns.values))\n",
    "print(\"Skew test:\", skewtest(current_pred_returns.values, alternative=\"two-sided\"))\n",
    "\n",
    "print(\"Kurtosis:\", kurtosis(current_pred_returns.values))\n",
    "print(\"Kurtosis test:\", kurtosistest(current_pred_returns.values, alternative=\"two-sided\"))\n",
    "\n",
    "print(\"Comparison normal:\", normaltest(comparison_returns))\n",
    "print(\"Pred normal:\", normaltest(current_pred_returns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ae4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using only mean returns on dates\n",
    "#Predicted\n",
    "\n",
    "current_digest = all_digests_nogroup\n",
    "\n",
    "current_region = \"us\"\n",
    "\n",
    "min_date = pd.Timestamp(\"2003-01-01\")\n",
    "max_date = pd.Timestamp(\"2023-12-31\")\n",
    "\n",
    "current_digest = all_digests_nogroup[all_digests_nogroup[\"model\"] == \"all_ensemble\"]\n",
    "current_digest = current_digest[current_digest[\"region\"] == current_region]\n",
    "current_digest = current_digest[current_digest[\"min_max_quantile\"] == (0.78, 1)]\n",
    "current_digest = current_digest[current_digest[\"date\"] >= min_date]\n",
    "current_digest = current_digest[current_digest[\"date\"] <= max_date]\n",
    "\n",
    "all_dates = True\n",
    "adaptive_model = True\n",
    "\n",
    "long_short_every_week = False\n",
    "\n",
    "compare_with_all_dates = False\n",
    "\n",
    "\n",
    "current_train_file = \"all_dates\"\n",
    "test_date_file_name = \"\"\n",
    "\n",
    "feature = \"top_minus_bottom_4_trr_5_fwd_ar_mean\"\n",
    "\n",
    "current_train_files = [\n",
    "    \"non_bear_dates_sp500\", \n",
    "    \"non_bear_dates_sp500\", \n",
    "    #\"all_dates\",\n",
    "    \"non_bear_dates_sp500\"\n",
    "                      ]\n",
    "\n",
    "final_train_file = \"non_bear_dates_sp500\"\n",
    "\n",
    "test_date_file_names = [\n",
    "    \"nber_recession_dates_class_lstm_ba4da75c\", \n",
    "    #\"bull_lstm_mc_change_class\"\n",
    "    \"return_filter_bear_m_short_2_3\", \n",
    "    \"bear_dates_qbear_class_lstm_9046df4a\",\n",
    "    #\"non_bull_dates_bqull_class_lstm_f241ab59\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "if all_dates:\n",
    "    current_pred = current_digest[current_digest[\"train_file\"] == current_train_file]\n",
    "\n",
    "elif adaptive_model:\n",
    "    for i in range(len(current_train_files)):\n",
    "        \n",
    "        test_dates = pd.read_csv(f\"../time_periods/model_test_ready/{test_date_file_names[i]}.csv\")\n",
    "        test_dates[\"date\"] = pd.to_datetime(test_dates[\"date\"])\n",
    "        if i != 0:\n",
    "            test_dates = test_dates[~test_dates[\"date\"].isin(current_pred[\"date\"])]\n",
    "        current_current_digest = current_digest[current_digest[\"date\"].isin(test_dates[\"date\"])]\n",
    "        current_current_digest = current_current_digest[current_current_digest[\"train_file\"] == current_train_files[i]]\n",
    "        current_current_digest[\"train_file\"] = current_train_files[i]\n",
    "        current_current_digest[\"test_file\"] = test_date_file_names[i]\n",
    "        if i == 0:\n",
    "            current_pred = current_current_digest\n",
    "        else:\n",
    "            current_pred = pd.concat([current_pred, current_current_digest])\n",
    "            \n",
    "        print(f\"Length adaptive state {i}:\", current_current_digest[feature].shape)\n",
    "        print(f\"Returns adaptive state {i}:\", current_current_digest[feature].mean())\n",
    "        \n",
    "    if long_short_every_week:\n",
    "        current_current_digest = current_digest[current_digest[\"train_file\"] == final_train_file]\n",
    "        current_current_digest = current_current_digest[~current_current_digest[\"date\"].isin(current_pred[\"date\"])]\n",
    "        current_pred = pd.concat([current_pred, current_current_digest])\n",
    "        print(f\"Length adaptive state {len(current_train_files)}:\", current_current_digest[feature].shape)\n",
    "        print(f\"Returns adaptive state {len(current_train_files)}:\", current_current_digest[feature].mean())\n",
    "        \n",
    "    current_pred.sort_values(\"date\", inplace=True)\n",
    "    \n",
    "\n",
    "    \n",
    "else:\n",
    "    test_dates = pd.read_csv(f\"../time_periods/model_test_ready/{test_date_file_name}.csv\")\n",
    "    test_dates[\"date\"] = pd.to_datetime(test_dates[\"date\"])\n",
    "    current_digest = current_digest[current_digest[\"date\"].isin(test_dates[\"date\"])]\n",
    "\n",
    "    current_pred = current_digest[current_digest[\"train_file\"] == current_train_file]\n",
    "\n",
    "\n",
    "current_pred_returns = current_pred.set_index(\"date\")[feature]\n",
    "\n",
    "if current_region == \"us\":\n",
    "    current_index = us_lookup_mc_cap_avg\n",
    "elif current_region == \"eu\":\n",
    "    current_index = eu_lookup_mc_cap_avg\n",
    "elif current_region == \"jp\":\n",
    "    current_index = jp_lookup_mc_cap_avg\n",
    "\n",
    "comparison_returns = current_digest[current_digest[\"train_file\"] == \"non_bear_dates_sp500\"].set_index(\"date\")[feature]\n",
    "\n",
    "if compare_with_all_dates:\n",
    "    comparison_returns = comparison_returns[comparison_returns.index >= min_date]\n",
    "    comparison_returns = comparison_returns[comparison_returns.index <= max_date]\n",
    "else:  \n",
    "    comparison_returns = comparison_returns[comparison_returns.index.isin(current_pred_returns.index)]\n",
    "\n",
    "print(\"Comparison length:\", comparison_returns.shape)\n",
    "print(\"Pred length:\", current_pred_returns.shape)\n",
    "\n",
    "print(\"Comparison return:\", comparison_returns.mean()*100)\n",
    "print(\"Pred return:\", current_pred_returns.mean()*100)\n",
    "print(\"Rel pred return:\", current_pred_returns.mean()*100 - comparison_returns.mean()*100)\n",
    "\n",
    "print(\"Comparison std:\", comparison_returns.std())\n",
    "print(\"Pred std:\", current_pred_returns.std())\n",
    "\n",
    "print(\"Comparison normal:\", normaltest(comparison_returns))\n",
    "print(\"Pred normal:\", normaltest(current_pred_returns))\n",
    "\n",
    "print(\"Ind:\", ttest_ind(current_pred_returns, comparison_returns, equal_var=False, alternative=\"greater\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086579b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name_dict = {\n",
    "    'nber_recession_dates' : \"NBER Recession\",\n",
    "    'nber_expansion_dates' : \"NBER Expansion\",\n",
    "    'bear_dates_sp500' : \"Qualitative Bear\",\n",
    "    'bull_dates_sp500' : \"Qualitative Bull\",\n",
    "    'non_bear_dates_sp500' : \"Qualitative Non-Bear\",\n",
    "    'return_filter_bear_m_long_3_6_12' : \"Negative Filter (LT)\",\n",
    "    'return_filter_bear_m_short_2_3' : \"Negative Filter (LT)\",\n",
    "    'return_filter_bull_m_long_3_6_12' : \"Positive Filter (LT)\",\n",
    "    'return_filter_bull_m_short_2_3' : \"Positive Filter (ST)\",\n",
    "    'EPU_rec_2yr' : \"EPU Recession\",\n",
    "    'EPU_exp_2yr' : \"EPU Expansion\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using only mean returns on dates\n",
    "#In-sample classification\n",
    "\n",
    "current_digest = all_digests_nogroup\n",
    "\n",
    "current_region = \"jp\"\n",
    "\n",
    "min_date = pd.Timestamp(\"2003-01-01\")\n",
    "max_date = pd.Timestamp(\"2023-12-31\")\n",
    "\n",
    "current_digest = all_digests_nogroup[all_digests_nogroup[\"model\"] == \"all_ensemble\"]\n",
    "current_digest = current_digest[current_digest[\"region\"] == current_region]\n",
    "current_digest = current_digest[current_digest[\"min_max_quantile\"] == (0.78, 1)]\n",
    "current_digest = current_digest[current_digest[\"date\"] >= min_date]\n",
    "current_digest = current_digest[current_digest[\"date\"] <= max_date]\n",
    "\n",
    "all_dates = False\n",
    "adaptive_model = True\n",
    "\n",
    "long_short_every_week = False\n",
    "\n",
    "compare_with_all_dates = False\n",
    "\n",
    "\n",
    "current_train_file = \"all_dates\"\n",
    "test_date_file_name = \"\"\n",
    "\n",
    "feature = \"top_minus_bottom_4_trr_5_fwd_ar_mean\"\n",
    "\n",
    "current_train_files = [\n",
    "    #'all_dates',\n",
    "    #'nber_recession_dates',\n",
    "    #'nber_expansion_dates',\n",
    "    #'bear_dates_sp500',\n",
    "    'bull_dates_sp500',\n",
    "    #'non_bear_dates_sp500',\n",
    "    #'return_filter_bear_m_long_3_6_12',\n",
    "    #'return_filter_bear_m_short_2_3',\n",
    "    #'return_filter_bull_m_long_3_6_12',\n",
    "    #'return_filter_bull_m_short_2_3',\n",
    "    #'EPU_rec_2yr',\n",
    "    #'EPU_exp_2yr'\n",
    "                      ]\n",
    "\n",
    "final_train_file = \"non_bear_dates_sp500\"\n",
    "\n",
    "test_date_file_names = [\n",
    "    #'nber_recession_dates',\n",
    "    'nber_expansion_dates',\n",
    "    #'bear_dates_sp500',\n",
    "    #'bull_dates_sp500',\n",
    "    #'non_bear_dates_sp500',\n",
    "    #'return_filter_bear_m_long_3_6_12',\n",
    "    #'return_filter_bear_m_short_2_3',\n",
    "    #'return_filter_bull_m_long_3_6_12',\n",
    "    #'return_filter_bull_m_short_2_3',\n",
    "    #'EPU_rec_2yr',\n",
    "    #'EPU_exp_2yr'\n",
    "]\n",
    "\n",
    "compare_train_file = 'bear_dates_sp500'\n",
    "\n",
    "\n",
    "\n",
    "if all_dates:\n",
    "    current_pred = current_digest[current_digest[\"train_file\"] == current_train_files[0]]\n",
    "\n",
    "elif adaptive_model:\n",
    "    for i in range(len(current_train_files)):\n",
    "        \n",
    "        test_dates = pd.read_csv(f\"../time_periods/model_train_ready/{test_date_file_names[i]}.csv\")\n",
    "        test_dates[\"date\"] = pd.to_datetime(test_dates[\"date\"])\n",
    "        if i != 0:\n",
    "            test_dates = test_dates[~test_dates[\"date\"].isin(current_pred[\"date\"])]\n",
    "        current_current_digest = current_digest[current_digest[\"date\"].isin(test_dates[\"date\"])]\n",
    "        current_current_digest = current_current_digest[current_current_digest[\"train_file\"] == current_train_files[i]]\n",
    "        current_current_digest[\"train_file\"] = current_train_files[i]\n",
    "        current_current_digest[\"test_file\"] = test_date_file_names[i]\n",
    "        if i == 0:\n",
    "            current_pred = current_current_digest\n",
    "        else:\n",
    "            current_pred = pd.concat([current_pred, current_current_digest])\n",
    "            \n",
    "        print(f\"Length adaptive state {i}:\", current_current_digest[feature].shape)\n",
    "        print(f\"Returns adaptive state {i}:\", current_current_digest[feature].mean())\n",
    "        \n",
    "    if long_short_every_week:\n",
    "        current_current_digest = current_digest[current_digest[\"train_file\"] == final_train_file]\n",
    "        current_current_digest = current_current_digest[~current_current_digest[\"date\"].isin(current_pred[\"date\"])]\n",
    "        current_pred = pd.concat([current_pred, current_current_digest])\n",
    "        print(f\"Length adaptive state {len(current_train_files)}:\", current_current_digest[feature].shape)\n",
    "        print(f\"Returns adaptive state {len(current_train_files)}:\", current_current_digest[feature].mean())\n",
    "        \n",
    "    current_pred.sort_values(\"date\", inplace=True)\n",
    "    \n",
    "\n",
    "    \n",
    "else:\n",
    "    test_dates = pd.read_csv(f\"../time_periods/model_test_ready/{test_date_file_name}.csv\")\n",
    "    test_dates[\"date\"] = pd.to_datetime(test_dates[\"date\"])\n",
    "    current_digest = current_digest[current_digest[\"date\"].isin(test_dates[\"date\"])]\n",
    "\n",
    "    current_pred = current_digest[current_digest[\"train_file\"] == current_train_file]\n",
    "\n",
    "\n",
    "current_pred_returns = current_pred.set_index(\"date\")[feature]\n",
    "\n",
    "if current_region == \"us\":\n",
    "    current_index = us_lookup_mc_cap_avg\n",
    "elif current_region == \"eu\":\n",
    "    current_index = eu_lookup_mc_cap_avg\n",
    "elif current_region == \"jp\":\n",
    "    current_index = jp_lookup_mc_cap_avg\n",
    "\n",
    "comparison_returns = current_digest[current_digest[\"train_file\"] == compare_train_file].set_index(\"date\")[feature]\n",
    "\n",
    "if compare_with_all_dates:\n",
    "    comparison_returns = comparison_returns[comparison_returns.index >= min_date]\n",
    "    comparison_returns = comparison_returns[comparison_returns.index <= max_date]\n",
    "else:  \n",
    "    comparison_returns = comparison_returns[comparison_returns.index.isin(current_pred_returns.index)]\n",
    "\n",
    "print(\"Comparison length:\", comparison_returns.shape)\n",
    "print(\"Pred length:\", current_pred_returns.shape)\n",
    "\n",
    "#print(\"Comparison return:\", comparison_returns.mean()*100)\n",
    "#print(\"Pred return:\", current_pred_returns.mean()*100)\n",
    "print(\"Rel pred return:\", current_pred_returns.mean()*100 - comparison_returns.mean()*100)\n",
    "\n",
    "#print(\"Comparison std:\", comparison_returns.std())\n",
    "#print(\"Pred std:\", current_pred_returns.std())\n",
    "\n",
    "#print(\"Comparison normal:\", normaltest(comparison_returns))\n",
    "#print(\"Pred normal:\", normaltest(current_pred_returns))\n",
    "\n",
    "print(\"Ind:\", ttest_ind(current_pred_returns, comparison_returns, equal_var=False, alternative=\"greater\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c963df",
   "metadata": {},
   "source": [
    "### Test indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f902014",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = pd.read_parquet(\"../data/indicators/US/all_indicators_raw_outer.parquet\", engine=\"pyarrow\")\n",
    "indicators[\"date\"] = pd.to_datetime(indicators[\"date\"])\n",
    "indicators.reset_index(drop=True, inplace=True)\n",
    "us_top_500 = pd.read_parquet(\"../data/indicators/US/us_top_500.parquet\", engine=\"pyarrow\")\n",
    "us_top_500[\"date\"] = pd.to_datetime(us_top_500[\"date\"])\n",
    "data = pd.merge(indicators, us_top_500, on=[\"date\"], how=\"outer\")\n",
    "data.set_index(\"date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc5f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"unemployment_change\"] = data[\"unemployment\"].dropna().pct_change()\n",
    "data[\"initial_claims_change\"] = data[\"initial_claims\"].dropna().pct_change()\n",
    "data[\"trr_w_fri\"] = data[\"market_cap_usd\"].resample(\"W-FRI\").last().pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name_dict = {\n",
    "    'nber_recession_dates_class_lstm_ba4da75c' : \"Recession Class LSTM\",\n",
    "    'nber_non_recession_dates_class_lstm_ba4da75c' : \"Non-Recession Class LSTM\",\n",
    "    'bear_dates_qbear_class_lstm_9046df4a' : \"Bear Class LSTM\",\n",
    "    'bull_dates_qbull_class_lstm_f241ab59' : \"Bull Class LSTM\",\n",
    "    'non_bear_dates_qbear_class_lstm_9046df4a' : \"Non-Bear Class LSTM\",\n",
    "    'non_bull_dates_bqull_class_lstm_f241ab59' : \"Non-Bull Class LSTM\",\n",
    "    'markov_rec_dates_test_all_years_order1_4_10_5yr_avg' : \"Markov Recession\",\n",
    "    'markov_exp_dates_test_all_years_order1_4_10_5yr_avg' : \"Markov Expansion\",\n",
    "    'return_filter_bull_m_long_3_6_12' : \"Positive Filter (LT)\",\n",
    "    'return_filter_bull_m_short_2_3' : \"Positive Filter (ST)\",\n",
    "    'bear_lstm_mc_change_class' : \"Negative MC Change LSTM\",\n",
    "    'bull_lstm_mc_change_class' : \"Positive MC Change LSTM\",\n",
    "    'return_filter_bear_m_long_3_6_12' : \"Negative Filter (LT)\",\n",
    "    'return_filter_bear_m_short_2_3' : \"Negative Filter (ST)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e2345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_region = \"us\"\n",
    "\n",
    "test_macros = True\n",
    "\n",
    "test_train_files = False\n",
    "\n",
    "min_date = pd.Timestamp(\"1980-01-01\")\n",
    "max_date = pd.Timestamp(\"2023-12-31\")\n",
    "\n",
    "feature = \"trr_w_fri\"\n",
    "\n",
    "\n",
    "\n",
    "test_date_file_names = [\n",
    "    #\"nber_recession_dates_class_lstm_ba4da75c\", \n",
    "    #\"markov_rec_dates_test_all_years_order1_4_10_5yr_avg\",\n",
    "    #\"bull_lstm_mc_change_class\"\n",
    "    #\"return_filter_bear_m_short_2_3\", \n",
    "    \"bear_dates_qbear_class_lstm_9046df4a\"\n",
    "]\n",
    "\n",
    "test_date_file_names = list(test_file_name_dict.keys())\n",
    "\n",
    "train_date_file_names = [\n",
    "    \"nber_recession_dates\", \n",
    "]\n",
    "\n",
    "if test_train_files:\n",
    "    for i in range(len(train_date_file_names)):\n",
    "\n",
    "        current_test_dates = pd.read_csv(f\"../time_periods/model_train_ready/{train_date_file_names[i]}.csv\")\n",
    "        current_test_dates[\"date\"] = pd.to_datetime(current_test_dates[\"date\"])\n",
    "        if i != 0:\n",
    "            current_test_dates = current_test_dates[~current_test_dates[\"date\"].isin(test_dates[\"date\"])]\n",
    "\n",
    "        if i == 0:\n",
    "            test_dates = current_test_dates\n",
    "        else:\n",
    "            test_dates = pd.concat([test_dates, current_test_dates])\n",
    "\n",
    "        print(f\"Length test file {i}:\", current_test_dates.shape)\n",
    "    \n",
    "else:  \n",
    "    for i in range(len(test_date_file_names)):\n",
    "\n",
    "        current_test_dates = pd.read_csv(f\"../time_periods/model_test_ready/{test_date_file_names[i]}.csv\")\n",
    "        current_test_dates[\"date\"] = pd.to_datetime(current_test_dates[\"date\"])\n",
    "        if i != 0:\n",
    "            current_test_dates = current_test_dates[~current_test_dates[\"date\"].isin(test_dates[\"date\"])]\n",
    "\n",
    "        if i == 0:\n",
    "            test_dates = current_test_dates\n",
    "        else:\n",
    "            test_dates = pd.concat([test_dates, current_test_dates])\n",
    "\n",
    "        print(f\"Length test file {i}:\", current_test_dates.shape)\n",
    "\n",
    "    \n",
    "if test_macros:\n",
    "    current_index = data[feature].dropna()\n",
    "    \n",
    "else:\n",
    "    if current_region == \"us\":\n",
    "        current_index = us_lookup_mc_cap_avg\n",
    "    elif current_region == \"eu\":\n",
    "        current_index = eu_lookup_mc_cap_avg\n",
    "    elif current_region == \"jp\":\n",
    "        current_index = jp_lookup_mc_cap_avg\n",
    "    \n",
    "current_index = current_index[current_index.index >= min_date]\n",
    "current_index = current_index[current_index.index <= max_date]\n",
    "    \n",
    "current_chosen_values = current_index.copy()[current_index.index.isin(test_dates[\"date\"])]\n",
    "\n",
    "comparison_values = current_index.copy()\n",
    "\n",
    "#print(\"Comparison length:\", comparison_values.shape)\n",
    "#print(\"Pred length:\", current_chosen_values.shape)\n",
    "\n",
    "#print(\"Comparison mean value:\", comparison_values.mean()*100)\n",
    "#print(\"Pred mean value:\", current_chosen_values.mean()*100)\n",
    "#print(\"Rel mean value:\", current_chosen_values.mean()*100 - comparison_values.mean()*100)\n",
    "\n",
    "#print(\"Comparison std:\", comparison_values.std())\n",
    "#print(\"Pred std:\", current_chosen_values.std())\n",
    "\n",
    "#print(\"Comparison normal:\", normaltest(comparison_values))\n",
    "#print(\"Pred normal:\", normaltest(current_chosen_values))\n",
    "\n",
    "\n",
    "print(\"Ind:\", ttest_ind(current_chosen_values, comparison_values, equal_var=False, alternative = \"less\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53cfe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name_dict = {\n",
    "    'nber_recession_dates' : \"NBER Recession\",\n",
    "    'nber_expansion_dates' : \"NBER Expansion\",\n",
    "    'bear_dates_sp500' : \"Qualitative Bear\",\n",
    "    'bull_dates_sp500' : \"Qualitative Bull\",\n",
    "    'non_bear_dates_sp500' : \"Qualitative Non-Bear\",\n",
    "    'return_filter_bear_m_long_3_6_12' : \"Negative Filter (LT)\",\n",
    "    'return_filter_bear_m_short_2_3' : \"Negative Filter (LT)\",\n",
    "    'return_filter_bull_m_long_3_6_12' : \"Positive Filter (LT)\",\n",
    "    'return_filter_bull_m_short_2_3' : \"Positive Filter (ST)\",\n",
    "    'EPU_rec_2yr' : \"EPU Recession\",\n",
    "    'EPU_exp_2yr' : \"EPU Expansion\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c011bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name_alternative_dict = {\n",
    "    'nber_recession_dates' : \"negative\",\n",
    "    'nber_expansion_dates' : \"positive\",\n",
    "    'bear_dates_sp500' : \"negative\",\n",
    "    'bull_dates_sp500' : \"positive\",\n",
    "    'non_bear_dates_sp500' : \"positive\",\n",
    "    'return_filter_bear_m_long_3_6_12' : \"negative\",\n",
    "    'return_filter_bear_m_short_2_3' : \"negative\",\n",
    "    'return_filter_bull_m_long_3_6_12' : \"positive\",\n",
    "    'return_filter_bull_m_short_2_3' : \"positive\",\n",
    "    'EPU_rec_2yr' : \"negative\",\n",
    "    'EPU_exp_2yr' : \"positive\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d265a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name_alternative_dict = {\n",
    "    'nber_recession_dates_class_lstm_ba4da75c' : \"negative\",\n",
    "    'nber_non_recession_dates_class_lstm_ba4da75c' : \"positive\",\n",
    "    'bear_dates_qbear_class_lstm_9046df4a' : \"negative\",\n",
    "    'bull_dates_qbull_class_lstm_f241ab59' : \"positive\",\n",
    "    'non_bear_dates_qbear_class_lstm_9046df4a' : \"positive\",\n",
    "    'non_bull_dates_bqull_class_lstm_f241ab59' : \"negative\",\n",
    "    'markov_rec_dates_test_all_years_order1_4_10_5yr_avg' : \"negative\",\n",
    "    'markov_exp_dates_test_all_years_order1_4_10_5yr_avg' : \"positive\",\n",
    "    'return_filter_bull_m_long_3_6_12' : \"positive\",\n",
    "    'return_filter_bull_m_short_2_3' : \"positive\",\n",
    "    'bear_lstm_mc_change_class' : \"negative\",\n",
    "    'bull_lstm_mc_change_class' : \"positive\",\n",
    "    'return_filter_bear_m_long_3_6_12' : \"negative\",\n",
    "    'return_filter_bear_m_short_2_3' : \"negative\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1198092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot all indicator t-test results\n",
    "\n",
    "current_region = \"us\"\n",
    "\n",
    "test_macros = True\n",
    "\n",
    "min_date = pd.Timestamp(\"1967-01-01\")\n",
    "max_date = pd.Timestamp(\"2019-12-31\")\n",
    "\n",
    "feature = \"trr_w_fri\"\n",
    "\n",
    "negative_is_less = True\n",
    "\n",
    "test_date_file_names = list(train_name_dict.keys())\n",
    "\n",
    "\n",
    "for i in range(len(test_date_file_names)):\n",
    "\n",
    "    current_test_dates = pd.read_csv(f\"../time_periods/model_train_ready/{test_date_file_names[i]}.csv\")\n",
    "    current_test_dates[\"date\"] = pd.to_datetime(current_test_dates[\"date\"])\n",
    "    \n",
    "    if test_macros:\n",
    "        current_index = data[feature].dropna()\n",
    "\n",
    "    else:\n",
    "        if current_region == \"us\":\n",
    "            current_index = us_lookup_mc_cap_avg\n",
    "        elif current_region == \"eu\":\n",
    "            current_index = eu_lookup_mc_cap_avg\n",
    "        elif current_region == \"jp\":\n",
    "            current_index = jp_lookup_mc_cap_avg\n",
    "\n",
    "    current_index = current_index[current_index.index >= min_date]\n",
    "    current_index = current_index[current_index.index <= max_date]\n",
    "\n",
    "    current_chosen_values = current_index.copy()[current_index.index.isin(current_test_dates[\"date\"])]\n",
    "\n",
    "    comparison_values = current_index.copy()\n",
    "    \n",
    "    if i == 0:\n",
    "        print(feature)\n",
    "        print(min_date, max_date)\n",
    "        print(\"Comparison length:\", comparison_values.shape)\n",
    "        print(\"Comparison mean value:\", comparison_values.mean()*100)\n",
    "        print()\n",
    "        \n",
    "    print(train_name_dict[test_date_file_names[i]])\n",
    "    print(\"Pred % chosen:\", 100*current_chosen_values.shape[0]/comparison_values.shape[0])\n",
    "\n",
    "    print(\"Rel mean value:\", current_chosen_values.mean()*100 - comparison_values.mean()*100)\n",
    "    \n",
    "    if train_name_alternative_dict[test_date_file_names[i]] == \"negative\":\n",
    "        if negative_is_less:\n",
    "            alternative = \"less\"\n",
    "        else:\n",
    "            alternative = \"greater\"\n",
    "            \n",
    "    else:\n",
    "        if negative_is_less:\n",
    "            alternative = \"greater\"\n",
    "        else:\n",
    "            alternative = \"less\"\n",
    "            \n",
    "    print(alternative)\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"T-value:\", ttest_ind(current_chosen_values, comparison_values, equal_var=False, alternative = alternative).statistic)\n",
    "    print(\"P-value:\", ttest_ind(current_chosen_values, comparison_values, equal_var=False, alternative = alternative).pvalue)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
