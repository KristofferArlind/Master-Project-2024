{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, uuid\n",
    "os.system(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = pd.read_parquet(\"../../data/indicators/US/all_indicators_raw_outer.parquet\", engine=\"pyarrow\")\n",
    "indicators[\"date\"] = pd.to_datetime(indicators[\"date\"])\n",
    "indicators.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nber_recessions = pd.read_parquet(\"../../data/indicators/US/nber_recession.parquet\")\n",
    "nber_recessions[\"date\"] = pd.to_datetime(nber_recessions[\"date\"])\n",
    "nber_recessions = nber_recessions[nber_recessions[\"date\"] >= \"1962-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_top_500 = pd.read_parquet(\"../../data/indicators/US/us_top_500.parquet\", engine=\"pyarrow\")\n",
    "us_top_500[\"date\"] = pd.to_datetime(us_top_500[\"date\"])\n",
    "data = pd.merge(indicators, us_top_500, on=[\"date\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index(\"date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ism_prod\"] = data[\"ISM_prod_index\"].copy()\n",
    "data[\"vix\"] = data[\"vix_SP500_close\"].copy()\n",
    "data[\"inflation\"] = data[\"inflation\"]/100\n",
    "data.loc[data.index < pd.Timestamp(\"1997-01-01\"), \"dvps_12m\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shifting appropriate date periods, daily data:\n",
    "data[\"vix\"] = data[\"vix\"].shift(1, freq = \"D\")\n",
    "data[\"market_cap_usd\"] = data[\"market_cap_usd\"].shift(1, freq = \"D\")\n",
    "data[\"credit_spread\"] = data[\"credit_spread\"].shift(1, freq = \"D\")\n",
    "data[\"rate_fed_funds\"] = data[\"rate_fed_funds\"].shift(1, freq = \"D\")\n",
    "data[\"rate_1_year\"] = data[\"rate_1_year\"].shift(1, freq = \"D\")\n",
    "data[\"rate_3_year\"] = data[\"rate_3_year\"].shift(1, freq = \"D\")\n",
    "data[\"rate_5_year\"] = data[\"rate_5_year\"].shift(1, freq = \"D\")\n",
    "data[\"rate_10_year\"] = data[\"rate_10_year\"].shift(1, freq = \"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shifting appropriate date periods, weekly data:\n",
    "data[\"initial_claims\"] = data[\"initial_claims\"].dropna().shift(1, freq = \"W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shifting appropriate date periods, monthly and quarterly data:\n",
    "data[\"real_gnp\"] = data[\"real_gnp\"].dropna().shift(3 + 2, freq = \"MS\")\n",
    "data[\"real_gdp\"] = data[\"real_gdp\"].dropna().shift(3 + 2, freq = \"MS\")\n",
    "data[\"M1\"] = data[\"M1\"].dropna().shift(1, freq = \"MS\")\n",
    "data[\"M2\"] = data[\"M2\"].dropna().shift(1, freq = \"MS\")\n",
    "data[\"ism_prod\"] = data[\"ism_prod\"].resample(\"ME\").mean().shift(1, freq=\"D\")\n",
    "data[\"pce\"] = data[\"pce\"].dropna().shift(1, freq = \"MS\").shift(7, freq = \"D\")\n",
    "data[\"unemployment\"] = data[\"unemployment\"].dropna().shift(2, freq = \"MS\")\n",
    "data[\"earnings_yield\"] = data[\"earnings_yield_12m\"].dropna().shift(-1, freq = \"D\").resample(\"QE\").last().shift(1, freq = \"D\").shift(2, freq=\"MS\")\n",
    "data[\"dividend_yield\"] = data[\"dividend_yield_12m\"].dropna().shift(0, freq = \"MS\")\n",
    "data[\"eps\"] = data[\"eps_12m\"].dropna().shift(-1, freq = \"D\").resample(\"QE\").last().shift(1, freq = \"D\").shift(2, freq=\"MS\")\n",
    "data[\"dvps\"] = data[\"dvps_12m\"].dropna().shift(0, freq = \"MS\")\n",
    "data[\"inflation\"] = data[\"inflation\"].dropna().shift(2, freq = \"MS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data is resampled to month-end and added one day to, so the first day of the month is information from last month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daily data, resample to monthly, pct_change\n",
    "data[\"vix_change\"] = data[\"vix\"].resample(\"ME\").mean().shift(1, freq=\"D\").dropna().pct_change()\n",
    "data[\"mc_change\"] = data[\"market_cap_usd\"].resample(\"ME\").mean().shift(1, freq=\"D\").dropna().pct_change()\n",
    "data[\"credit_spread_change\"] = data[\"credit_spread\"].resample(\"ME\").mean().shift(1, freq=\"D\").dropna().pct_change()\n",
    "data[\"rate_fed_funds_change\"] = data[\"rate_fed_funds\"].resample(\"ME\").mean().shift(1, freq=\"D\").dropna().pct_change()\n",
    "data[\"rate_1_year_change\"] = data[\"rate_1_year\"].resample(\"ME\").mean().shift(1, freq=\"D\").dropna().pct_change()\n",
    "data[\"rate_3_year_change\"] = data[\"rate_3_year\"].resample(\"ME\").mean().shift(1, freq=\"D\").dropna().pct_change()\n",
    "data[\"rate_5_year_change\"] = data[\"rate_5_year\"].resample(\"ME\").mean().shift(1, freq=\"D\").dropna().pct_change()\n",
    "data[\"rate_10_year_change\"] = data[\"rate_10_year\"].resample(\"ME\").mean().shift(1, freq=\"D\").dropna().pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weekly data, resample to monthly, pct_change\n",
    "data[\"initial_claims_change\"] = data[\"initial_claims\"].resample(\"ME\").mean().shift(1, freq=\"D\").dropna().pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monthly data, pct_change\n",
    "data[\"real_gnp_change\"] = data[\"real_gnp\"].dropna().pct_change()\n",
    "data[\"real_gdp_change\"] = data[\"real_gdp\"].dropna().pct_change()\n",
    "data[\"m1_change\"] = data[\"M1\"].dropna().pct_change()\n",
    "data[\"m2_change\"] = data[\"M2\"].dropna().pct_change()\n",
    "data[\"ism_prod_change\"] = data[\"ism_prod\"].dropna().pct_change()\n",
    "data[\"pce_change\"] = data[\"pce\"].dropna().pct_change()\n",
    "data[\"unemployment_change\"] = data[\"unemployment\"].dropna().pct_change()\n",
    "data[\"earnings_yield_change\"] = data[\"earnings_yield\"].dropna().pct_change()\n",
    "data[\"dividend_yield_change\"] = data[\"dividend_yield\"].dropna().pct_change()\n",
    "data[\"eps_change\"] = data[\"eps\"].dropna().pct_change()\n",
    "data[\"dvps_change\"] = data[\"dvps\"].dropna().pct_change()\n",
    "data[\"inflation_change\"] = data[\"inflation\"].dropna().pct_change()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, lookback):\n",
    "    Xs = []\n",
    "    for i in range(len(X) - lookback + 1):\n",
    "        Xs.append(X[i:(i + lookback)])\n",
    "    return np.array(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, features, target_data, train_test_split_date, lookback, n_test_periods = None, embargo_periods=1, scale_data = False):\n",
    "\n",
    "    X_data = data[features]\n",
    "\n",
    "    y_data = target_data\n",
    "\n",
    "    \n",
    "    X_test = X_data[X_data.index >= train_test_split_date]\n",
    "    y_test = y_data[y_data.index >= train_test_split_date]\n",
    "\n",
    "    X_train = X_data[X_data.index < train_test_split_date]\n",
    "    y_train = y_data[y_data.index < train_test_split_date]\n",
    "\n",
    "    if (scale_data):\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        scaler.fit(X_train)\n",
    "\n",
    "        X_train.loc[:,features] = scaler.transform(X_train)\n",
    "        X_test.loc[:,features] = scaler.transform(X_test)\n",
    "\n",
    "    \n",
    "    X_train_seq = create_sequences(X_train.values, lookback)\n",
    "    y_train_seq = y_train.values[lookback - 1:]\n",
    "\n",
    "    if n_test_periods:\n",
    "        X_test = X_test.iloc[:n_test_periods]\n",
    "        X_test_seq = create_sequences(pd.concat([X_train.iloc[-lookback + 1:], X_test]).values, lookback)\n",
    "        y_test_seq = y_test.values[lookback - 1:]\n",
    "        y_test = y_test.iloc[:n_test_periods]\n",
    "        y_test_seq = np.concatenate([y_train_seq[-lookback + n_test_periods + 1:], y_test_seq])\n",
    "\n",
    "    else:\n",
    "        X_test_seq = create_sequences(X_test.values, lookback)\n",
    "        X_test_seq = np.concatenate([X_train_seq[-lookback + 1:], X_test_seq])\n",
    "    \n",
    "        y_test_seq = y_test.values[lookback - 1:]\n",
    "        y_test_seq = np.concatenate([y_train_seq[-lookback + 1:], y_test_seq])\n",
    "        \n",
    "    test_dates = X_test.index\n",
    "    train_dates = X_train.iloc[lookback - 1:].index\n",
    "    \n",
    "    if embargo_periods:\n",
    "        train_dates = train_dates[:-embargo_periods]\n",
    "        X_train_seq = X_train_seq[:-embargo_periods]\n",
    "        y_train_seq = y_train_seq[:-embargo_periods]\n",
    "\n",
    "    return X_train_seq, y_train_seq, X_test_seq, y_test_seq, X_train, X_test, y_train, y_test, train_dates, test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def process_data(data, feature, winsorize_std = 3, winsorize_quantile=None, scale_data=False, log_transform=False, plot=False):\n",
    "    assert(not (winsorize_std and winsorize_quantile))\n",
    "    \n",
    "    data_train = data[[feature]].copy().dropna()\n",
    "\n",
    "    if winsorize_quantile is not None:\n",
    "        data_train[feature] = data_train[feature].clip(lower = data_train[feature].quantile(winsorize_quantile), upper = data_train[feature].quantile(1-winsorize_quantile))\n",
    "\n",
    "    if winsorize_std is not None:\n",
    "        data_train[feature] = data_train[feature].clip(lower = -data_train[feature].std()*winsorize_std, upper = data_train[feature].std()*winsorize_std)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    if log_transform:\n",
    "        if (data_train[feature].min() + 1) <= 0:\n",
    "            print(data_train[feature].min())\n",
    "            print(f\"Feature {feature} has too negative values, cannot log transform\")\n",
    "            return\n",
    "        data_train[feature] = np.log(1 + data_train[feature])\n",
    "\n",
    "    if scale_data:\n",
    "        data_train[feature] = scaler.fit_transform(data_train[feature].values.reshape(-1, 1))\n",
    "\n",
    "    if plot:\n",
    "        print(feature)\n",
    "        data_train[feature].plot()\n",
    "        plt.show()\n",
    "\n",
    "    return data_train[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = pd.Timestamp(\"1962-01-01\")\n",
    "max_date = pd.Timestamp(\"2023-12-31\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "                \"mc_change\", \n",
    "                \"inflation\",\n",
    "                #\"inflation_change\",\n",
    "                #\"unemployment\", \n",
    "                \"unemployment_change\", \n",
    "                #\"rate_fed_funds\",\n",
    "                \"rate_fed_funds_change\", \n",
    "                \"initial_claims_change\",\n",
    "                #\"ism_prod_index\",\n",
    "                \"ism_prod_change\",\n",
    "                \"real_gnp_change\", \n",
    "                \"real_gdp_change\", \n",
    "                \"m1_change\", \n",
    "                \"m2_change\", \n",
    "                #\"rate_1_year\",\n",
    "                #\"rate_3_year\",\n",
    "                #\"rate_5_year\",\n",
    "                #\"rate_10_year\",\n",
    "                \"rate_1_year_change\",\n",
    "                \"rate_3_year_change\",\n",
    "                \"rate_5_year_change\",\n",
    "                \"rate_10_year_change\",\n",
    "                #\"earnings_yield\",\n",
    "                \"earnings_yield_change\",\n",
    "                \"eps_change\",\n",
    "                #\"dvps_change\",\n",
    "                #\"credit_spread\",\n",
    "                \"credit_spread_change\",\n",
    "                #\"pce_change\",\n",
    "                #\"vix_change\"\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    print(feature, str(data[feature].dropna().index.min()), str(data[feature].dropna().index.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_freq = \"MS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    data_copy[feature] = process_data(data_copy, feature, winsorize_std = 3, winsorize_quantile=None, scale_data=True, log_transform=True, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resample to monthly\n",
    "for feature in features:\n",
    "    data_copy[feature] = data_copy[feature].resample(resample_freq).first().ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data_copy[data_copy.index >= min_date]\n",
    "data_copy = data_copy[data_copy.index <= max_date]\n",
    "data_copy = data_copy[features].dropna()\n",
    "print(data_copy.index.min())\n",
    "print(data_copy.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use recession months as class 1\n",
    "#PROBLEMATIC PERIOD, NEEDS TO BE RETRAINED AT SPECIFIC TIMES\n",
    "\n",
    "y_data = nber_recessions.copy()\n",
    "y_data.set_index(\"date\", inplace=True)\n",
    "y_data[\"class\"] = y_data[\"recession\"]\n",
    "y_data = y_data.resample(resample_freq).sum()\n",
    "y_data[\"class\"] = y_data[\"class\"].apply(lambda x: 1 if x >= 15 else 0)\n",
    "y_data = y_data[y_data.index >= data_copy.index.min()]\n",
    "y_data = y_data[y_data.index <= data_copy.index.max()]\n",
    "print(y_data.index.min())\n",
    "print(y_data.index.max())\n",
    "y_data = y_data[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bear_dates = pd.read_csv(\"../../time_periods/bear_dates_sp500.csv\", engine=\"pyarrow\")\n",
    "bear_dates[\"date\"] = pd.to_datetime(bear_dates[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use bear dates as class 1\n",
    "#PROBLEMATIC PERIOD, NEEDS TO BE RETRAINED AT SPECIFIC TIMES\n",
    "\n",
    "y_data = bear_dates.copy()\n",
    "y_data.set_index(\"date\", inplace=True)\n",
    "y_data[\"class\"] = 1\n",
    "y_data = y_data.resample(\"D\").asfreq().fillna(0)\n",
    "y_data = y_data.resample(resample_freq).sum()\n",
    "y_data[\"class\"] = y_data[\"class\"].apply(lambda x: 1 if x > 15 else 0)\n",
    "y_data = y_data[y_data.index >= data_copy.index.min()]\n",
    "y_data = y_data[y_data.index <= data_copy.index.max()]\n",
    "print(y_data.index.min())\n",
    "print(y_data.index.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bull_dates = pd.read_csv(\"../../time_periods/bull_dates_sp500.csv\", engine=\"pyarrow\")\n",
    "bull_dates[\"date\"] = pd.to_datetime(bull_dates[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use bull dates as class 1\n",
    "#PROBLEMATIC PERIOD, NEEDS TO BE RETRAINED AT SPECIFIC TIMES\n",
    "\n",
    "y_data = bull_dates.copy()\n",
    "y_data.set_index(\"date\", inplace=True)\n",
    "y_data[\"class\"] = 1\n",
    "y_data = y_data.resample(\"D\").asfreq().fillna(0)\n",
    "y_data = y_data.resample(resample_freq).sum()\n",
    "y_data[\"class\"] = y_data[\"class\"].apply(lambda x: 1 if x > 15 else 0)\n",
    "y_data = y_data[y_data.index >= data_copy.index.min()]\n",
    "y_data = y_data[y_data.index <= data_copy.index.max()]\n",
    "print(y_data.index.min())\n",
    "print(y_data.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use negative change in market cap as class 1\n",
    "y_data = data_copy[[\"mc_change\"]].dropna().shift(-1)\n",
    "y_data = y_data[y_data.index >= data_copy.index.min()]\n",
    "y_data = y_data[y_data.index <= data_copy.index.max()]\n",
    "print(y_data.index.min())\n",
    "print(y_data.index.max())\n",
    "y_data[\"class\"] = 0\n",
    "y_data.loc[y_data[\"mc_change\"] < 0, \"class\"] = 1\n",
    "y_data = y_data[\"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different compositions of the LSTM model is used. See the thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEAR, BULL, AND RECESSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing one model composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(1000, return_sequences=True, input_shape=(lookback, len(features))))\n",
    "model.add(LSTM(1000, return_sequences=False, input_shape=(lookback, len(features))))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for negative market probability\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model after before each date in list:\n",
    "\n",
    "train_split_dates_nber = [pd.Timestamp(\"1981-07-08\"), pd.Timestamp(\"1983-07-08\"), pd.Timestamp(\"1992-12-22\"), \n",
    "                        pd.Timestamp(\"2003-07-17\"), pd.Timestamp(\"2010-09-20\")]\n",
    "\n",
    "\n",
    "train_split_dates_bear = [pd.Timestamp(\"1983-02-15\"), pd.Timestamp(\"1985-02-01\"), pd.Timestamp(\"1988-06-07\"), \n",
    "                        pd.Timestamp(\"1991-04-10\"), pd.Timestamp(\"1999-04-10\"), pd.Timestamp(\"2000-11-25\"),\n",
    "                        pd.Timestamp(\"2003-09-12\"), pd.Timestamp(\"2009-09-10\"), pd.Timestamp(\"2011-01-10\"),  \n",
    "                        pd.Timestamp(\"2012-04-04\"), pd.Timestamp(\"2016-08-12\"), pd.Timestamp(\"2019-06-28\"), \n",
    "                        #  pd.Timestamp(\"2020-09-28\"), pd.Timestamp(\"2023-04-15\"), pd.Timestamp(\"2024-05-01\"), \n",
    "                         ]\n",
    "\n",
    "train_split_dates_bull = [pd.Timestamp(\"1983-02-15\"), pd.Timestamp(\"1985-02-01\"), pd.Timestamp(\"1988-06-07\"), \n",
    "                        pd.Timestamp(\"1991-04-10\"), pd.Timestamp(\"1999-04-10\"), pd.Timestamp(\"2000-11-25\"),\n",
    "                        pd.Timestamp(\"2003-09-12\"), pd.Timestamp(\"2009-09-10\"), pd.Timestamp(\"2011-01-10\"),  \n",
    "                        pd.Timestamp(\"2012-04-04\"), pd.Timestamp(\"2016-08-12\"), pd.Timestamp(\"2019-06-28\"), \n",
    "                        #  pd.Timestamp(\"2020-09-28\"), pd.Timestamp(\"2023-04-15\"), pd.Timestamp(\"2024-05-01\"), \n",
    "                         ]\n",
    "\n",
    "train_split_dates = train_split_dates_bear\n",
    "\n",
    "\n",
    "\n",
    "train_split_dates_next_month = [date + pd.offsets.MonthBegin(0) for date in train_split_dates]\n",
    "\n",
    "for i, current_train_test_split_date in enumerate(train_split_dates_next_month):\n",
    "    \n",
    "    print(current_train_test_split_date)\n",
    "    X_train_seq, y_train_seq, X_test_seq, y_test_seq, X_train, X_test, y_train, y_test, train_dates, test_dates = prepare_data(data_copy,\n",
    "        features, y_data, current_train_test_split_date, lookback = lookback)\n",
    "\n",
    "    num_0 = len(y_train[y_train == 0])\n",
    "    num_1 = len(y_train[y_train == 1])\n",
    "    num_both = len(y_train)\n",
    "\n",
    "    weight_0 = (1 / num_0) * (num_both / 2)\n",
    "    weight_1 = (1 / num_1) * (num_both / 2)\n",
    "\n",
    "    class_weights = {0: weight_0, 1: weight_1}\n",
    "\n",
    "    model.fit(X_train_seq, y_train_seq, epochs=40, verbose=0, class_weight=class_weights)\n",
    "\n",
    "    train_results = model.predict(X_train_seq)\n",
    "    test_results = model.predict(X_test_seq)\n",
    "    \n",
    "    print(train_dates)\n",
    "    print(train_dates[-1])\n",
    "    \n",
    "    print(test_results)\n",
    "    print(test_dates)\n",
    "\n",
    "    if i == 0:\n",
    "        new_test_results_df = pd.DataFrame(test_results, index=test_dates, columns=[\"p\"])\n",
    "        new_test_results_df[\"split_date\"] = current_train_test_split_date\n",
    "        new_test_results_df[\"real_class\"] = y_data[y_data.index.isin(new_test_results_df.index)]\n",
    "        test_results_df = new_test_results_df\n",
    "    else:\n",
    "        new_test_results_df = pd.DataFrame(test_results, index=test_dates, columns=[\"p\"])\n",
    "        new_test_results_df[\"split_date\"] = current_train_test_split_date\n",
    "        new_test_results_df[\"real_class\"] = y_data[y_data.index.isin(new_test_results_df.index)]\n",
    "        test_results_df = pd.concat([test_results_df, new_test_results_df])\n",
    "\n",
    "    print(\"New test results df:\", new_test_results_df.shape)\n",
    "\n",
    "test_results_df[\"pred_class\"] = 1\n",
    "test_results_df.loc[test_results_df[\"p\"] < 0.5, \"pred_class\"] = 0\n",
    "\n",
    "test_results_df_seq = test_results_df.copy()\n",
    "for i, date in enumerate(train_split_dates_next_month):\n",
    "    if (i + 1) < len(train_split_dates_next_month):\n",
    "        test_results_df_seq.loc[test_results_df_seq[\"split_date\"] == date] = test_results_df_seq[(test_results_df_seq[\"split_date\"] == date) & (test_results_df_seq.index < train_split_dates_next_month[i+1])]\n",
    "        test_results_df_seq.dropna(inplace=True)\n",
    "        \n",
    "\n",
    "    \n",
    "save_results = True\n",
    "\n",
    "if save_results:\n",
    "    model_id = uuid.uuid4()\n",
    "\n",
    "    #model_name = f\"nber_recession_class_train_before_1980_win_std_3_scale_log_retrained_{str(model_id)[:8]}\"\n",
    "    #model_name = f\"qbull_class_train_before_1980_win_std_3_scale_log_retrained_{str(model_id)[:8]}\"\n",
    "    model_name = f\"qbear_class_train_before_1980_win_std_3_scale_log_retrained_{str(model_id)[:8]}\"\n",
    "\n",
    "    directory = f\"../../results/regime/lstm/{model_name}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    test_results_df.to_csv(f\"{directory}/test_results.csv\")\n",
    "    test_results_df_seq.to_csv(f\"{directory}/test_results_seq.csv\")\n",
    "    with open(f\"{directory}/summary.txt\", 'w') as f:\n",
    "        model.summary(print_fn=lambda x: f.write(x + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing multiple compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model after before each date in list:\n",
    "\n",
    "train_split_dates_nber = [pd.Timestamp(\"1981-07-08\"), pd.Timestamp(\"1983-07-08\"), pd.Timestamp(\"1992-12-22\"), \n",
    "                        pd.Timestamp(\"2003-07-17\"), pd.Timestamp(\"2010-09-20\")]\n",
    "\n",
    "\n",
    "train_split_dates_bear = [pd.Timestamp(\"1975-04-01\"), pd.Timestamp(\"1983-02-15\"), pd.Timestamp(\"1985-02-01\"), pd.Timestamp(\"1988-06-07\"), \n",
    "                        pd.Timestamp(\"1991-04-10\"), pd.Timestamp(\"1999-04-10\"), pd.Timestamp(\"2000-11-25\"),\n",
    "                        pd.Timestamp(\"2003-09-12\"), pd.Timestamp(\"2009-09-10\"), pd.Timestamp(\"2011-01-10\"),  \n",
    "                        pd.Timestamp(\"2012-04-04\"), pd.Timestamp(\"2016-08-12\"), pd.Timestamp(\"2019-06-28\"), \n",
    "                        pd.Timestamp(\"2020-09-28\"), pd.Timestamp(\"2023-04-15\"), pd.Timestamp(\"2024-05-01\"), \n",
    "                         ]\n",
    "\n",
    "train_split_dates_bull = [\n",
    "    pd.Timestamp('1976-01-15'), pd.Timestamp('1981-05-26'), pd.Timestamp('1983-12-23'),\n",
    "    pd.Timestamp('1988-02-25'), pd.Timestamp('1990-04-09'), pd.Timestamp('1994-08-01'),\n",
    "    pd.Timestamp('1999-01-17'),pd.Timestamp('2000-09-24'),pd.Timestamp('2004-09-01'),\n",
    "    pd.Timestamp('2005-08-28'),pd.Timestamp('2006-11-08'),pd.Timestamp('2008-01-17'),\n",
    "    pd.Timestamp('2010-10-23'),pd.Timestamp('2011-11-02'),pd.Timestamp('2015-11-19'),\n",
    "    pd.Timestamp('2018-07-25'),pd.Timestamp('2019-03-20'),pd.Timestamp('2020-08-19'),\n",
    "    pd.Timestamp('2022-07-03')\n",
    "                         ]\n",
    "\n",
    "train_split_dates = train_split_dates_bear\n",
    "\n",
    "lookback_list = [2,5]\n",
    "dense_scale_list = [0.5,0.75,1,1.25,1.5,2]\n",
    "last_layer_list = [True, False]\n",
    "\n",
    "for lookback in lookback_list:\n",
    "    for dense_scale in dense_scale_list:\n",
    "        for last_layer in last_layer_list:\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(1000, return_sequences=True, input_shape=(lookback, len(features))))\n",
    "            model.add(LSTM(1000, return_sequences=False, input_shape=(lookback, len(features))))\n",
    "            model.add(Dense(int(dense_scale*1000), activation='relu'))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(int(dense_scale*500), activation='relu'))\n",
    "            model.add(Dense(int(dense_scale*250), activation='relu'))\n",
    "            model.add(Dense(int(dense_scale*100), activation='relu'))\n",
    "            if last_layer:\n",
    "                model.add(Dense(int(dense_scale*50), activation='relu'))\n",
    "            model.add(Dense(1, activation='sigmoid'))  # Output layer for negative market probability\n",
    "            model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "            train_split_dates_next_month = [date + pd.offsets.MonthBegin(0) for date in train_split_dates]\n",
    "\n",
    "            for i, current_train_test_split_date in enumerate(train_split_dates_next_month):\n",
    "\n",
    "                print(current_train_test_split_date)\n",
    "                X_train_seq, y_train_seq, X_test_seq, y_test_seq, X_train, X_test, y_train, y_test, train_dates, test_dates = prepare_data(data_copy,\n",
    "                    features, y_data, current_train_test_split_date, lookback = lookback)\n",
    "\n",
    "                num_0 = len(y_train[y_train == 0])\n",
    "                num_1 = len(y_train[y_train == 1])\n",
    "                num_both = len(y_train)\n",
    "\n",
    "                weight_0 = (1 / num_0) * (num_both / 2)\n",
    "                weight_1 = (1 / num_1) * (num_both / 2)\n",
    "\n",
    "                class_weights = {0: weight_0, 1: weight_1}\n",
    "\n",
    "                model.fit(X_train_seq, y_train_seq, epochs=40, verbose=0, class_weight=class_weights)\n",
    "\n",
    "                train_results = model.predict(X_train_seq)\n",
    "                test_results = model.predict(X_test_seq)\n",
    "\n",
    "                print(train_dates)\n",
    "                print(train_dates[-1])\n",
    "\n",
    "                print(test_results)\n",
    "                print(test_dates)\n",
    "\n",
    "                if i == 0:\n",
    "                    new_test_results_df = pd.DataFrame(test_results, index=test_dates, columns=[\"p\"])\n",
    "                    new_test_results_df[\"split_date\"] = current_train_test_split_date\n",
    "                    new_test_results_df[\"real_class\"] = y_data[y_data.index.isin(new_test_results_df.index)]\n",
    "                    test_results_df = new_test_results_df\n",
    "                else:\n",
    "                    new_test_results_df = pd.DataFrame(test_results, index=test_dates, columns=[\"p\"])\n",
    "                    new_test_results_df[\"split_date\"] = current_train_test_split_date\n",
    "                    new_test_results_df[\"real_class\"] = y_data[y_data.index.isin(new_test_results_df.index)]\n",
    "                    test_results_df = pd.concat([test_results_df, new_test_results_df])\n",
    "\n",
    "                print(\"New test results df:\", new_test_results_df.shape)\n",
    "\n",
    "            test_results_df[\"pred_class\"] = 1\n",
    "            test_results_df.loc[test_results_df[\"p\"] < 0.5, \"pred_class\"] = 0\n",
    "\n",
    "            test_results_df_seq = test_results_df.copy()\n",
    "            for i, date in enumerate(train_split_dates_next_month):\n",
    "                if (i + 1) < len(train_split_dates_next_month):\n",
    "                    test_results_df_seq.loc[test_results_df_seq[\"split_date\"] == date] = test_results_df_seq[(test_results_df_seq[\"split_date\"] == date) & (test_results_df_seq.index < train_split_dates_next_month[i+1])]\n",
    "                    test_results_df_seq.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "            save_results = True\n",
    "\n",
    "            if save_results:\n",
    "                model_id = uuid.uuid4()\n",
    "\n",
    "                #model_name = f\"nber_recession_class_train_before_1980_win_std_3_scale_log_retrained_{str(model_id)[:8]}\"\n",
    "                #model_name = f\"qbull_class_train_before_1980_win_std_3_scale_log_retrained_{str(model_id)[:8]}\"\n",
    "\n",
    "                model_name = f\"qbear_class_train_before_1980_win_std_3_scale_log_retrained_{str(model_id)[:8]}\"\n",
    "\n",
    "                directory = f\"../../results/regime/lstm/{model_name}\"\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                test_results_df.to_csv(f\"{directory}/test_results.csv\")\n",
    "                test_results_df_seq.to_csv(f\"{directory}/test_results_seq.csv\")\n",
    "                with open(f\"{directory}/summary.txt\", 'w') as f:\n",
    "                    model.summary(print_fn=lambda x: f.write(x + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Market Cap change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model for each freq\n",
    "time_between_training = 1 #months\n",
    "embargo_months = 1\n",
    "\n",
    "train_test_split_date = pd.Timestamp(\"1980-01-01\")\n",
    "\n",
    "current_train_test_split_date = train_test_split_date\n",
    "\n",
    "while current_train_test_split_date + pd.DateOffset(months=time_between_training) < max_date:\n",
    "    \n",
    "    print(current_train_test_split_date)\n",
    "    X_train_seq, y_train_seq, X_test_seq, y_test_seq, X_train, X_test, y_train, y_test, train_dates, test_dates = prepare_data(data_copy,\n",
    "        features, y_data, current_train_test_split_date, lookback = lookback, n_test_periods=time_between_training)\n",
    "\n",
    "    num_0 = len(y_train[y_train == 0])\n",
    "    num_1 = len(y_train[y_train == 1])\n",
    "    num_both = len(y_train)\n",
    "\n",
    "    weight_0 = (1 / num_0) * (num_both / 2)\n",
    "    weight_1 = (1 / num_1) * (num_both / 2)\n",
    "\n",
    "    class_weights = {0: weight_0, 1: weight_1}\n",
    "\n",
    "    model.fit(X_train_seq, y_train_seq, epochs=40, verbose=0, class_weight=class_weights)\n",
    "\n",
    "    train_results = model.predict(X_train_seq)\n",
    "    test_results = model.predict(X_test_seq)\n",
    "    \n",
    "    print(train_dates)\n",
    "    print(train_dates[-1])\n",
    "    \n",
    "    print(test_results)\n",
    "    print(test_dates)\n",
    "\n",
    "    if current_train_test_split_date == train_test_split_date:\n",
    "        train_results_df = pd.DataFrame(train_results, index=train_dates, columns=[\"p\"])\n",
    "        new_test_results_df = pd.DataFrame(test_results, index=test_dates, columns=[\"p\"])\n",
    "        test_results_df = new_test_results_df\n",
    "    else:\n",
    "        new_test_results_df = pd.DataFrame(test_results, index=test_dates, columns=[\"p\"])\n",
    "        test_results_df = pd.concat([test_results_df, new_test_results_df])\n",
    "\n",
    "    print(\"New test results df:\", new_test_results_df.shape)\n",
    "\n",
    "    current_train_test_split_date = current_train_test_split_date + pd.DateOffset(months=time_between_training)\n",
    "\n",
    "test_results_df[\"pred_class\"] = 1\n",
    "test_results_df.loc[test_results_df[\"p\"] < 0.5, \"pred_class\"] = 0\n",
    "test_results_df[\"real_class\"] = y_data[y_data.index.isin(test_results_df.index)]\n",
    "\n",
    "train_results_df[\"pred_class\"] = 1\n",
    "train_results_df.loc[train_results_df[\"p\"] < 0.5, \"pred_class\"] = 0\n",
    "\n",
    "all_results = pd.concat([train_results_df, test_results_df])\n",
    "\n",
    "model_id = uuid.uuid4()\n",
    "\n",
    "model_name = f\"mc_change_class_train_before_1980_win_std_3_scale_log_retrained_{str(model_id)[:8]}\"\n",
    "\n",
    "directory = f\"../../results/regime/lstm/{model_name}\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "test_results_df.to_csv(f\"{directory}/test_results.csv\")\n",
    "with open(f\"{directory}/summary.txt\", 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
